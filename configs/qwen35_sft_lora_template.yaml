# ZMLX SFT with LoRA (Qwen3.5 preparation template)
# Fill in official model IDs once released under the Qwen org on Hugging Face.
# Usage: zmlx train --config configs/qwen35_sft_lora_template.yaml

model: "<HF_ORG>/<QWEN3_5_MODEL_ID>"
dataset: "mlx-community/WikiSQL"

# LoRA
lora: true
lora_rank: 8
lora_alpha: 16.0
lora_dropout: 0.0
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj

# Training
iters: 400
batch_size: 2
learning_rate: 2.0e-4
lr_schedule: cosine
warmup_steps: 50
max_seq_length: 2048

# Evaluation
eval_interval: 100
save_interval: 200
output_dir: adapters/qwen35_sft_lora

# ZMLX patching
patch: true
patch_profile: qwen3
patch_compute_dtype: float32
patch_threadgroup: 256
