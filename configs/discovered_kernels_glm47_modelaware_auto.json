{
  "schema_version": "2",
  "runtime": {
    "mlx_version": "0.30.6.dev20260208+185b06d9",
    "device_name": "Apple M4 Max",
    "device_arch": "applegpu_g16s"
  },
  "entries": [
    {
      "key": {
        "op_name": "rmsnorm_residual",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 1,
          "D": 2048
        }
      },
      "candidate_id": "rmsnorm_residual_d6959400a3811a21",
      "func_name": "kk_kd_rmsnorm_res_d2048_tg512_v2_u1_simd0",
      "metal_source": "constexpr uint D = 2048;\nconstexpr uint TG = 512;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr float EPS = 1e-06f;\nconstexpr bool USE_SIMD = false;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduce_buf[TG];\n\nfloat sumsq = 0.0f;\nuint start = tid * VEC;\nuint step = TG * VEC;\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float x = (float)inp[base + idx] + (float)residual[base + idx];\n                updated_res[base + idx] = (T)x;\n                sumsq += x * x;\n            }\n        }\n    }\n}\n\nif (USE_SIMD) {\n    KK_SIMD_REDUCE_SUM(reduce_buf, sumsq, tid, TG);\n} else {\n    reduce_buf[tid] = sumsq;\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint stride = TG / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            reduce_buf[tid] += reduce_buf[tid + stride];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n}\n\nfloat inv = metal::rsqrt(reduce_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float vres = (float)updated_res[base + idx];\n                float w = (float)weight[idx];\n                out[base + idx] = (T)(vres * inv * w);\n            }\n        }\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "inp",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "residual",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "weight",
          "dtype": "float16",
          "shape": [
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "updated_res",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "use_simd": false,
        "eps": 1e-06
      },
      "launch_params": {
        "threadgroup_x": 512,
        "launch_kind": "rmsnorm_residual_rows_tg"
      },
      "source_hash": "1f6e3303f113729f845ca40b574619f8c961f2b5bad8def6c6378be14e247784",
      "metrics": {
        "latency_us": 126.833,
        "speedup_vs_ref": 1.6686982094565297,
        "correctness_max_abs_err": 0.001953125,
        "correctness_max_rel_err": 0.0009689922480620155
      }
    },
    {
      "key": {
        "op_name": "rmsnorm_residual",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 2,
          "D": 2048
        }
      },
      "candidate_id": "rmsnorm_residual_2e58f6828a2c7547",
      "func_name": "kk_kd_rmsnorm_res_d2048_tg512_v1_u4_simd0",
      "metal_source": "constexpr uint D = 2048;\nconstexpr uint TG = 512;\nconstexpr uint VEC = 1;\nconstexpr uint UNROLL = 4;\nconstexpr float EPS = 1e-06f;\nconstexpr bool USE_SIMD = false;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduce_buf[TG];\n\nfloat sumsq = 0.0f;\nuint start = tid * VEC;\nuint step = TG * VEC;\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float x = (float)inp[base + idx] + (float)residual[base + idx];\n                updated_res[base + idx] = (T)x;\n                sumsq += x * x;\n            }\n        }\n    }\n}\n\nif (USE_SIMD) {\n    KK_SIMD_REDUCE_SUM(reduce_buf, sumsq, tid, TG);\n} else {\n    reduce_buf[tid] = sumsq;\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint stride = TG / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            reduce_buf[tid] += reduce_buf[tid + stride];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n}\n\nfloat inv = metal::rsqrt(reduce_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float vres = (float)updated_res[base + idx];\n                float w = (float)weight[idx];\n                out[base + idx] = (T)(vres * inv * w);\n            }\n        }\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "inp",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "residual",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "weight",
          "dtype": "float16",
          "shape": [
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "updated_res",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 1,
        "unroll": 4,
        "use_simd": false,
        "eps": 1e-06
      },
      "launch_params": {
        "threadgroup_x": 512,
        "launch_kind": "rmsnorm_residual_rows_tg"
      },
      "source_hash": "d276769ce5344875e94c1f165d815107a431e8debed1562416bb45a176dc1110",
      "metrics": {
        "latency_us": 127.1245,
        "speedup_vs_ref": 1.5109833273680526,
        "correctness_max_abs_err": 0.001953125,
        "correctness_max_rel_err": 0.000970873786407767
      }
    },
    {
      "key": {
        "op_name": "rmsnorm_residual",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 4,
          "D": 2048
        }
      },
      "candidate_id": "rmsnorm_residual_2e58f6828a2c7547",
      "func_name": "kk_kd_rmsnorm_res_d2048_tg512_v1_u4_simd0",
      "metal_source": "constexpr uint D = 2048;\nconstexpr uint TG = 512;\nconstexpr uint VEC = 1;\nconstexpr uint UNROLL = 4;\nconstexpr float EPS = 1e-06f;\nconstexpr bool USE_SIMD = false;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduce_buf[TG];\n\nfloat sumsq = 0.0f;\nuint start = tid * VEC;\nuint step = TG * VEC;\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float x = (float)inp[base + idx] + (float)residual[base + idx];\n                updated_res[base + idx] = (T)x;\n                sumsq += x * x;\n            }\n        }\n    }\n}\n\nif (USE_SIMD) {\n    KK_SIMD_REDUCE_SUM(reduce_buf, sumsq, tid, TG);\n} else {\n    reduce_buf[tid] = sumsq;\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint stride = TG / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            reduce_buf[tid] += reduce_buf[tid + stride];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n}\n\nfloat inv = metal::rsqrt(reduce_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float vres = (float)updated_res[base + idx];\n                float w = (float)weight[idx];\n                out[base + idx] = (T)(vres * inv * w);\n            }\n        }\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "inp",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "residual",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "weight",
          "dtype": "float16",
          "shape": [
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "updated_res",
          "dtype": "float16",
          "shape": [
            1,
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 1,
        "unroll": 4,
        "use_simd": false,
        "eps": 1e-06
      },
      "launch_params": {
        "threadgroup_x": 512,
        "launch_kind": "rmsnorm_residual_rows_tg"
      },
      "source_hash": "d276769ce5344875e94c1f165d815107a431e8debed1562416bb45a176dc1110",
      "metrics": {
        "latency_us": 123.2915,
        "speedup_vs_ref": 1.6922983336239725,
        "correctness_max_abs_err": 0.00390625,
        "correctness_max_rel_err": 0.0009551098376313276
      }
    },
    {
      "key": {
        "op_name": "swiglu",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 1,
          "D": 1536,
          "N": 1536
        }
      },
      "candidate_id": "swiglu_b791c2fdfb054bcf",
      "func_name": "kk_kd_swiglu_n1536_tg128_v2_u1_fast1",
      "metal_source": "constexpr uint N = 1536;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr bool FAST_SIGMOID = true;\n\nuint tid = thread_position_in_grid.x;\nuint base = tid * VEC * UNROLL;\n\n#pragma unroll\nfor (uint u = 0; u < UNROLL; ++u) {\n    uint idx0 = base + u * VEC;\n    if (idx0 >= N) {\n        continue;\n    }\n\n    #pragma unroll\n    for (uint v = 0; v < VEC; ++v) {\n        uint idx = idx0 + v;\n        if (idx >= N) {\n            continue;\n        }\n\n        float g = (float)gate[idx];\n        float upv = (float)up[idx];\n        float sig;\n        if (FAST_SIGMOID) {\n            sig = kk_sigmoid(g);\n        } else {\n            sig = 1.0f / (1.0f + metal::exp(-g));\n        }\n        out[idx] = (T)(g * sig * upv);\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "gate",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        },
        {
          "name": "up",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "fast_sigmoid": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "swiglu_flat",
        "vec_width": 2,
        "unroll": 1
      },
      "source_hash": "6d62cd57d6ef57d597c13fbb94ffcdc99a3f2e364580b6626869132627a28293",
      "metrics": {
        "latency_us": 150.875,
        "speedup_vs_ref": 1.0223695111847555,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "swiglu",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 2,
          "D": 1536,
          "N": 3072
        }
      },
      "candidate_id": "swiglu_b791c2fdfb054bcf",
      "func_name": "kk_kd_swiglu_n1536_tg128_v2_u1_fast1",
      "metal_source": "constexpr uint N = 1536;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr bool FAST_SIGMOID = true;\n\nuint tid = thread_position_in_grid.x;\nuint base = tid * VEC * UNROLL;\n\n#pragma unroll\nfor (uint u = 0; u < UNROLL; ++u) {\n    uint idx0 = base + u * VEC;\n    if (idx0 >= N) {\n        continue;\n    }\n\n    #pragma unroll\n    for (uint v = 0; v < VEC; ++v) {\n        uint idx = idx0 + v;\n        if (idx >= N) {\n            continue;\n        }\n\n        float g = (float)gate[idx];\n        float upv = (float)up[idx];\n        float sig;\n        if (FAST_SIGMOID) {\n            sig = kk_sigmoid(g);\n        } else {\n            sig = 1.0f / (1.0f + metal::exp(-g));\n        }\n        out[idx] = (T)(g * sig * upv);\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "gate",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        },
        {
          "name": "up",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "fast_sigmoid": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "swiglu_flat",
        "vec_width": 2,
        "unroll": 1
      },
      "source_hash": "6d62cd57d6ef57d597c13fbb94ffcdc99a3f2e364580b6626869132627a28293",
      "metrics": {
        "latency_us": 142.9375,
        "speedup_vs_ref": 1.2229995627459553,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "swiglu",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 4,
          "D": 1536,
          "N": 6144
        }
      },
      "candidate_id": "swiglu_b791c2fdfb054bcf",
      "func_name": "kk_kd_swiglu_n1536_tg128_v2_u1_fast1",
      "metal_source": "constexpr uint N = 1536;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr bool FAST_SIGMOID = true;\n\nuint tid = thread_position_in_grid.x;\nuint base = tid * VEC * UNROLL;\n\n#pragma unroll\nfor (uint u = 0; u < UNROLL; ++u) {\n    uint idx0 = base + u * VEC;\n    if (idx0 >= N) {\n        continue;\n    }\n\n    #pragma unroll\n    for (uint v = 0; v < VEC; ++v) {\n        uint idx = idx0 + v;\n        if (idx >= N) {\n            continue;\n        }\n\n        float g = (float)gate[idx];\n        float upv = (float)up[idx];\n        float sig;\n        if (FAST_SIGMOID) {\n            sig = kk_sigmoid(g);\n        } else {\n            sig = 1.0f / (1.0f + metal::exp(-g));\n        }\n        out[idx] = (T)(g * sig * upv);\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "gate",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        },
        {
          "name": "up",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            1536
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "fast_sigmoid": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "swiglu_flat",
        "vec_width": 2,
        "unroll": 1
      },
      "source_hash": "6d62cd57d6ef57d597c13fbb94ffcdc99a3f2e364580b6626869132627a28293",
      "metrics": {
        "latency_us": 156.52100000000002,
        "speedup_vs_ref": 1.0901093144050957,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "rope",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "B": 1,
          "H_Q": 20,
          "D_NOPE": 192,
          "D_ROPE": 64,
          "D_OUT": 256
        }
      },
      "candidate_id": "rope_55f0cd083f9854b0",
      "func_name": "kk_kd_rope_dn192_dr64_hq20_tg128_fma0",
      "metal_source": "constexpr uint D_NOPE = 192;\nconstexpr uint D_ROPE = 64;\nconstexpr uint HALF = D_ROPE / 2;\nconstexpr uint D_OUT = D_NOPE + D_ROPE;\nconstexpr uint H_Q = 20;\nconstexpr bool USE_FMA = false;\n\nconstexpr uint Q_ELEMS_PER_BATCH = H_Q * D_OUT;\nconstexpr uint K_ELEMS_PER_BATCH = D_OUT;\nconstexpr uint ELEMS_PER_BATCH = Q_ELEMS_PER_BATCH + K_ELEMS_PER_BATCH;\n\nuint gid = thread_position_in_grid.x;\nuint batch = gid / ELEMS_PER_BATCH;\nuint in_batch = gid - batch * ELEMS_PER_BATCH;\n\nif (in_batch < Q_ELEMS_PER_BATCH) {\n    uint head = in_batch / D_OUT;\n    uint col = in_batch - head * D_OUT;\n\n    uint q_out_base = (batch * H_Q + head) * D_OUT;\n\n    if (col < D_NOPE) {\n        uint q_nope_base = (batch * H_Q + head) * D_NOPE;\n        q_out[q_out_base + col] = q_nope[q_nope_base + col];\n        return;\n    }\n\n    uint r = col - D_NOPE;\n    uint pair = r / 2;\n    float c = (float)cos[pair];\n    float s = (float)sin[pair];\n\n    uint q_rope_base = (batch * H_Q + head) * D_ROPE;\n    if ((r & 1u) == 0u) {\n        float a = (float)q_rope[q_rope_base + r];\n        float b = (float)q_rope[q_rope_base + r + 1];\n        float rotated = USE_FMA ? fma(-b, s, a * c) : (a * c - b * s);\n        q_out[q_out_base + col] = (T)rotated;\n    } else {\n        float a = (float)q_rope[q_rope_base + r - 1];\n        float b = (float)q_rope[q_rope_base + r];\n        float rotated = USE_FMA ? fma(b, c, a * s) : (a * s + b * c);\n        q_out[q_out_base + col] = (T)rotated;\n    }\n    return;\n}\n\nuint k_col = in_batch - Q_ELEMS_PER_BATCH;\nuint k_out_base = batch * D_OUT;\nif (k_col < D_NOPE) {\n    uint kv_nope_base = batch * D_NOPE;\n    k_out[k_out_base + k_col] = kv_nope[kv_nope_base + k_col];\n    return;\n}\n\nuint kr = k_col - D_NOPE;\nuint kpair = kr / 2;\nfloat kc = (float)cos[kpair];\nfloat ks = (float)sin[kpair];\nuint k_rope_base = batch * D_ROPE;\nif ((kr & 1u) == 0u) {\n    float a = (float)k_rope[k_rope_base + kr];\n    float b = (float)k_rope[k_rope_base + kr + 1];\n    float rotated = USE_FMA ? fma(-b, ks, a * kc) : (a * kc - b * ks);\n    k_out[k_out_base + k_col] = (T)rotated;\n} else {\n    float a = (float)k_rope[k_rope_base + kr - 1];\n    float b = (float)k_rope[k_rope_base + kr];\n    float rotated = USE_FMA ? fma(b, kc, a * ks) : (a * ks + b * kc);\n    k_out[k_out_base + k_col] = (T)rotated;\n}\n",
      "inputs_spec": [
        {
          "name": "q_nope",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "q_rope",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "kv_nope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_rope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "cos",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        },
        {
          "name": "sin",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "q_out",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            256
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_out",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            256
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "use_fma": false
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "rope_decode_pos"
      },
      "source_hash": "2f4ae6139153e737ac6ec703cf872bf4795492ee7951e8926adcb5c143d1e9b6",
      "metrics": {
        "latency_us": 132.83350000000002,
        "speedup_vs_ref": 1.6359766173442691,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "rope",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "B": 2,
          "H_Q": 20,
          "D_NOPE": 192,
          "D_ROPE": 64,
          "D_OUT": 256
        }
      },
      "candidate_id": "rope_7da7bc615da4af60",
      "func_name": "kk_kd_rope_dn192_dr64_hq20_tg128_fma1",
      "metal_source": "constexpr uint D_NOPE = 192;\nconstexpr uint D_ROPE = 64;\nconstexpr uint HALF = D_ROPE / 2;\nconstexpr uint D_OUT = D_NOPE + D_ROPE;\nconstexpr uint H_Q = 20;\nconstexpr bool USE_FMA = true;\n\nconstexpr uint Q_ELEMS_PER_BATCH = H_Q * D_OUT;\nconstexpr uint K_ELEMS_PER_BATCH = D_OUT;\nconstexpr uint ELEMS_PER_BATCH = Q_ELEMS_PER_BATCH + K_ELEMS_PER_BATCH;\n\nuint gid = thread_position_in_grid.x;\nuint batch = gid / ELEMS_PER_BATCH;\nuint in_batch = gid - batch * ELEMS_PER_BATCH;\n\nif (in_batch < Q_ELEMS_PER_BATCH) {\n    uint head = in_batch / D_OUT;\n    uint col = in_batch - head * D_OUT;\n\n    uint q_out_base = (batch * H_Q + head) * D_OUT;\n\n    if (col < D_NOPE) {\n        uint q_nope_base = (batch * H_Q + head) * D_NOPE;\n        q_out[q_out_base + col] = q_nope[q_nope_base + col];\n        return;\n    }\n\n    uint r = col - D_NOPE;\n    uint pair = r / 2;\n    float c = (float)cos[pair];\n    float s = (float)sin[pair];\n\n    uint q_rope_base = (batch * H_Q + head) * D_ROPE;\n    if ((r & 1u) == 0u) {\n        float a = (float)q_rope[q_rope_base + r];\n        float b = (float)q_rope[q_rope_base + r + 1];\n        float rotated = USE_FMA ? fma(-b, s, a * c) : (a * c - b * s);\n        q_out[q_out_base + col] = (T)rotated;\n    } else {\n        float a = (float)q_rope[q_rope_base + r - 1];\n        float b = (float)q_rope[q_rope_base + r];\n        float rotated = USE_FMA ? fma(b, c, a * s) : (a * s + b * c);\n        q_out[q_out_base + col] = (T)rotated;\n    }\n    return;\n}\n\nuint k_col = in_batch - Q_ELEMS_PER_BATCH;\nuint k_out_base = batch * D_OUT;\nif (k_col < D_NOPE) {\n    uint kv_nope_base = batch * D_NOPE;\n    k_out[k_out_base + k_col] = kv_nope[kv_nope_base + k_col];\n    return;\n}\n\nuint kr = k_col - D_NOPE;\nuint kpair = kr / 2;\nfloat kc = (float)cos[kpair];\nfloat ks = (float)sin[kpair];\nuint k_rope_base = batch * D_ROPE;\nif ((kr & 1u) == 0u) {\n    float a = (float)k_rope[k_rope_base + kr];\n    float b = (float)k_rope[k_rope_base + kr + 1];\n    float rotated = USE_FMA ? fma(-b, ks, a * kc) : (a * kc - b * ks);\n    k_out[k_out_base + k_col] = (T)rotated;\n} else {\n    float a = (float)k_rope[k_rope_base + kr - 1];\n    float b = (float)k_rope[k_rope_base + kr];\n    float rotated = USE_FMA ? fma(b, kc, a * ks) : (a * ks + b * kc);\n    k_out[k_out_base + k_col] = (T)rotated;\n}\n",
      "inputs_spec": [
        {
          "name": "q_nope",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "q_rope",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "kv_nope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_rope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "cos",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        },
        {
          "name": "sin",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "q_out",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            256
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_out",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            256
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "use_fma": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "rope_decode_pos"
      },
      "source_hash": "652c8d334148cce7a79d024ae729bb7de340689dc003a27f502258f2f94d4cc6",
      "metrics": {
        "latency_us": 131.54149999999998,
        "speedup_vs_ref": 1.603110045118841,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "rope",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "B": 4,
          "H_Q": 20,
          "D_NOPE": 192,
          "D_ROPE": 64,
          "D_OUT": 256
        }
      },
      "candidate_id": "rope_7da7bc615da4af60",
      "func_name": "kk_kd_rope_dn192_dr64_hq20_tg128_fma1",
      "metal_source": "constexpr uint D_NOPE = 192;\nconstexpr uint D_ROPE = 64;\nconstexpr uint HALF = D_ROPE / 2;\nconstexpr uint D_OUT = D_NOPE + D_ROPE;\nconstexpr uint H_Q = 20;\nconstexpr bool USE_FMA = true;\n\nconstexpr uint Q_ELEMS_PER_BATCH = H_Q * D_OUT;\nconstexpr uint K_ELEMS_PER_BATCH = D_OUT;\nconstexpr uint ELEMS_PER_BATCH = Q_ELEMS_PER_BATCH + K_ELEMS_PER_BATCH;\n\nuint gid = thread_position_in_grid.x;\nuint batch = gid / ELEMS_PER_BATCH;\nuint in_batch = gid - batch * ELEMS_PER_BATCH;\n\nif (in_batch < Q_ELEMS_PER_BATCH) {\n    uint head = in_batch / D_OUT;\n    uint col = in_batch - head * D_OUT;\n\n    uint q_out_base = (batch * H_Q + head) * D_OUT;\n\n    if (col < D_NOPE) {\n        uint q_nope_base = (batch * H_Q + head) * D_NOPE;\n        q_out[q_out_base + col] = q_nope[q_nope_base + col];\n        return;\n    }\n\n    uint r = col - D_NOPE;\n    uint pair = r / 2;\n    float c = (float)cos[pair];\n    float s = (float)sin[pair];\n\n    uint q_rope_base = (batch * H_Q + head) * D_ROPE;\n    if ((r & 1u) == 0u) {\n        float a = (float)q_rope[q_rope_base + r];\n        float b = (float)q_rope[q_rope_base + r + 1];\n        float rotated = USE_FMA ? fma(-b, s, a * c) : (a * c - b * s);\n        q_out[q_out_base + col] = (T)rotated;\n    } else {\n        float a = (float)q_rope[q_rope_base + r - 1];\n        float b = (float)q_rope[q_rope_base + r];\n        float rotated = USE_FMA ? fma(b, c, a * s) : (a * s + b * c);\n        q_out[q_out_base + col] = (T)rotated;\n    }\n    return;\n}\n\nuint k_col = in_batch - Q_ELEMS_PER_BATCH;\nuint k_out_base = batch * D_OUT;\nif (k_col < D_NOPE) {\n    uint kv_nope_base = batch * D_NOPE;\n    k_out[k_out_base + k_col] = kv_nope[kv_nope_base + k_col];\n    return;\n}\n\nuint kr = k_col - D_NOPE;\nuint kpair = kr / 2;\nfloat kc = (float)cos[kpair];\nfloat ks = (float)sin[kpair];\nuint k_rope_base = batch * D_ROPE;\nif ((kr & 1u) == 0u) {\n    float a = (float)k_rope[k_rope_base + kr];\n    float b = (float)k_rope[k_rope_base + kr + 1];\n    float rotated = USE_FMA ? fma(-b, ks, a * kc) : (a * kc - b * ks);\n    k_out[k_out_base + k_col] = (T)rotated;\n} else {\n    float a = (float)k_rope[k_rope_base + kr - 1];\n    float b = (float)k_rope[k_rope_base + kr];\n    float rotated = USE_FMA ? fma(b, kc, a * ks) : (a * ks + b * kc);\n    k_out[k_out_base + k_col] = (T)rotated;\n}\n",
      "inputs_spec": [
        {
          "name": "q_nope",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "q_rope",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "kv_nope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_rope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "cos",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        },
        {
          "name": "sin",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "q_out",
          "dtype": "float16",
          "shape": [
            1,
            20,
            1,
            256
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_out",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            256
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "use_fma": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "rope_decode_pos"
      },
      "source_hash": "652c8d334148cce7a79d024ae729bb7de340689dc003a27f502258f2f94d4cc6",
      "metrics": {
        "latency_us": 128.458,
        "speedup_vs_ref": 1.7186200937271325,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    }
  ]
}