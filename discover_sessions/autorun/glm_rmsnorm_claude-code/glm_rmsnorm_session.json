{
  "schema_version": "1.0",
  "metadata": {
    "session_id": "9fc64556386a4503",
    "target_name": "glm_rmsnorm",
    "llm_backend": "claude-code",
    "device_chip": "Apple M4",
    "device_memory_gb": 36,
    "os_version": "Darwin 25.1.0",
    "started_at": "2026-02-08T13:00:38-0600",
    "updated_at": "2026-02-08T13:06:39-0600",
    "total_steps": 10,
    "total_candidates": 40,
    "total_evaluated": 40,
    "best_reward": 1.1495834890142032,
    "best_speedup": 1.1595491576746533,
    "best_source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;  // 8\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float final_sum[1];\nfloat local_vals[ELEMS_PER_THREAD];\nfloat local_weights[ELEMS_PER_THREAD];\n\n// Level 1: Register accumulation (8 elements per thread)\nfloat sumsq = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    local_vals[k] = (float)inp[base + offset + k];\n    local_weights[k] = (float)weight[offset + k];\n    sumsq += local_vals[k] * local_vals[k];\n}\n\n// Level 2: SIMD reduction (32 lanes)\nfor (uint s = 16; s > 0; s >>= 1) {\n    sumsq += simd_shuffle_down(sumsq, s);\n}\n\n// Level 3: Threadgroup reduction (8 SIMD groups)\nthreadgroup float simd_results[TG / 32];\nuint lane = tid & 31;\nuint simd_idx = tid >> 5;\n\nif (lane == 0) {\n    simd_results[simd_idx] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid == 0) {\n    float total = 0.0f;\n    for (uint i = 0; i < TG / 32; i++) {\n        total += simd_results[i];\n    }\n    final_sum[0] = total;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(final_sum[0] / (float)D + EPS);\n\n// Direct register output\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(local_vals[k] * inv * local_weights[k]);\n}",
    "baseline_us": 144.75
  },
  "tree_data": {
    "c_puct": 1.0,
    "root": {
      "node_id": "root",
      "candidate": {
        "spec": {
          "name": "kk_rmsnorm_D2048",
          "input_names": [
            "inp",
            "weight"
          ],
          "output_names": [
            "out"
          ],
          "source": "\n        constexpr uint D = 2048;\n        constexpr uint TG = 256;\n        constexpr float EPS = 1e-6f;\n\n        uint gid = thread_position_in_grid.x;\n        uint tid = thread_position_in_threadgroup.x;\n        uint row = gid / TG;\n        uint base = row * D;\n\n        threadgroup float buf[TG];\n\n        float sumsq = 0.0f;\n        for (uint j = tid; j < D; j += TG) {\n            float v = (float)inp[base + j];\n            sumsq += v * v;\n        }\n        KK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\n        float inv = metal::rsqrt(buf[0] / (float)D + EPS);\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n\n        for (uint j = tid; j < D; j += TG) {\n            float v = (float)inp[base + j];\n            float w = (float)weight[j];\n            out[base + j] = (T)(v * inv * w);\n        }\n    ",
          "header": "",
          "threadgroup": [
            256,
            1,
            1
          ],
          "template_params": [
            [
              "T",
              "float32"
            ]
          ]
        },
        "parent_id": null,
        "generation": 0,
        "llm_reasoning": "baseline"
      },
      "visit_count": 41,
      "max_reward": 1.1495834890142032,
      "prior": 1.0,
      "eval_result": {
        "compiled": true,
        "correct": true,
        "compile_error": null,
        "correctness_error": null,
        "timings_us": [
          136.75,
          138.917,
          127.292,
          135.041,
          129.5,
          127.208,
          128.292,
          134.25,
          135.708,
          125.75,
          125.167,
          128.417,
          123.458,
          129.083,
          127.459,
          130.916,
          139.333,
          132.292,
          129.75,
          251.334
        ],
        "median_us": 129.75,
        "reward": 1.0719635530160734,
        "speedup": 1.115606936416185
      },
      "children": [
        {
          "node_id": "gen1_ed96515fe09c",
          "candidate": {
            "spec": {
              "name": "kk_glm_rmsnorm_gen1",
              "input_names": [
                "inp",
                "weight"
              ],
              "output_names": [
                "out"
              ],
              "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Vectorized sum-of-squares computation\nfloat sumsq = 0.0f;\nfor (uint j = tid * 4; j < D; j += TG * 4) {\n    float4 v = *((device float4*)&inp[base + j]);\n    sumsq += v.x * v.x + v.y * v.y + v.z * v.z + v.w * v.w;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Vectorized output computation\nfor (uint j = tid * 4; j < D; j += TG * 4) {\n    float4 v = *((device float4*)&inp[base + j]);\n    float4 w = *((device float4*)&weight[j]);\n    float4 result = v * inv * w;\n    *((device float4*)&out[base + j]) = (float4)((T)result.x, (T)result.y, (T)result.z, (T)result.w);\n}",
              "header": "",
              "threadgroup": [
                256,
                1,
                1
              ],
              "template_params": [
                [
                  "T",
                  "float32"
                ]
              ]
            },
            "parent_id": "root",
            "generation": 1,
            "llm_reasoning": "Vectorized loads/stores using float4 to increase memory bandwidth utilization. Each thread processes 4 elements per iteration."
          },
          "visit_count": 1,
          "max_reward": 0.0,
          "prior": 0.25,
          "eval_result": {
            "compiled": true,
            "correct": false,
            "compile_error": null,
            "correctness_error": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\nOutput 0 mismatch\nMismatched elements: 1536 / 2048 (75%)\nFirst 5 mismatches are at indices:\n [0, 0]: -0.022539563477039337 (ACTUAL), -2.82169508934021 (DESIRED)\n [0, 1]: -0.022539563477039337 (ACTUAL), -0.010000457055866718 (DESIRED)\n [0, 2]: -0.022539563477039337 (ACTUAL), -0.523980975151062 (DESIRED)\n [0, 4]: 0.09140914678573608 (ACTUAL), -1.4585134983062744 (DESIRED)\n [0, 5]: 0.09140914678573608 (ACTUAL), -0.21335050463676453 (DESIRED)\nMax absolute difference among violations: 8.133413\nMax relative difference among violations: 2133.508\n ACTUAL: array([[-0.02254 , -0.02254 , -0.02254 , ...,  0.730387,  0.730387,\n         0.730387]], shape=(1, 2048), dtype=float32)\n DESIRED: array([[-2.821695, -0.01    , -0.523981, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)",
            "timings_us": [],
            "median_us": Infinity,
            "reward": 0.0,
            "speedup": 0.0
          },
          "children": []
        },
        {
          "node_id": "gen1_8d1551be6855",
          "candidate": {
            "spec": {
              "name": "kk_glm_rmsnorm_gen1",
              "input_names": [
                "inp",
                "weight"
              ],
              "output_names": [
                "out"
              ],
              "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint UNROLL = 8;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * UNROLL) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    float v4 = (float)inp[base + j + TG*4];\n    float v5 = (float)inp[base + j + TG*5];\n    float v6 = (float)inp[base + j + TG*6];\n    float v7 = (float)inp[base + j + TG*7];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3 + v4*v4 + v5*v5 + v6*v6 + v7*v7;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j = tid; j < D; j += TG * UNROLL) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    float v4 = (float)inp[base + j + TG*4];\n    float v5 = (float)inp[base + j + TG*5];\n    float v6 = (float)inp[base + j + TG*6];\n    float v7 = (float)inp[base + j + TG*7];\n    \n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * (float)weight[j + TG]);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * (float)weight[j + TG*2]);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * (float)weight[j + TG*3]);\n    out[base + j + TG*4] = (T)((float)inp[base + j + TG*4] * inv * (float)weight[j + TG*4]);\n    out[base + j + TG*5] = (T)((float)inp[base + j + TG*5] * inv * (float)weight[j + TG*5]);\n    out[base + j + TG*6] = (T)((float)inp[base + j + TG*6] * inv * (float)weight[j + TG*6]);\n    out[base + j + TG*7] = (T)((float)inp[base + j + TG*7] * inv * (float)weight[j + TG*7]);\n}",
              "header": "",
              "threadgroup": [
                256,
                1,
                1
              ],
              "template_params": [
                [
                  "T",
                  "float32"
                ]
              ]
            },
            "parent_id": "root",
            "generation": 1,
            "llm_reasoning": "Loop unrolling with 8x unroll factor to increase instruction-level parallelism and reduce loop overhead."
          },
          "visit_count": 21,
          "max_reward": 1.1495834890142032,
          "prior": 0.25,
          "eval_result": {
            "compiled": true,
            "correct": true,
            "compile_error": null,
            "correctness_error": null,
            "timings_us": [
              141.083,
              138.0,
              147.042,
              167.792,
              159.209,
              155.166,
              155.875,
              154.417,
              153.25,
              150.917,
              141.208,
              154.542,
              152.041,
              144.375,
              256.625,
              139.0,
              135.042,
              127.917,
              122.375,
              122.875
            ],
            "median_us": 150.917,
            "reward": 0.9710917119542104,
            "speedup": 0.9591364789917637
          },
          "children": [
            {
              "node_id": "gen2_adb8f750d639",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen2",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\n// SIMD-level reduction first\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction across SIMD groups\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_8d1551be6855",
                "generation": 2,
                "llm_reasoning": "Use SIMD-level reduction with simd_sum intrinsic before threadgroup reduction to reduce shared memory traffic and synchronization overhead."
              },
              "visit_count": 5,
              "max_reward": 0.9665419246364743,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": true,
                "compile_error": null,
                "correctness_error": null,
                "timings_us": [
                  162.541,
                  143.042,
                  142.5,
                  139.833,
                  136.584,
                  146.125,
                  150.709,
                  152.0,
                  158.417,
                  154.75,
                  155.125,
                  153.083,
                  158.75,
                  158.083,
                  156.541,
                  146.5,
                  263.291,
                  153.458,
                  155.959,
                  185.292
                ],
                "median_us": 154.75,
                "reward": 0.9219359270876253,
                "speedup": 0.9353796445880452
              },
              "children": [
                {
                  "node_id": "gen5_b893f97c23b4",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen5",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Sum-of-squares with 4-way unroll\nfloat sumsq = 0.0f;\nuint j = tid;\nfor (; j + TG*3 < D; j += TG*4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n// Remainder\nfor (; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\n// SIMD reduction\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Apply normalization with 4-way unroll\nj = tid;\nfor (; j + TG*3 < D; j += TG*4) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * (float)weight[j + TG]);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * (float)weight[j + TG*2]);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * (float)weight[j + TG*3]);\n}\nfor (; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_adb8f750d639",
                    "generation": 5,
                    "llm_reasoning": "Use 4-way manual unrolling with register rotation to maximize ILP while maintaining coalesced access and keeping loop overhead minimal. Each thread processes 4 elements per iteration."
                  },
                  "visit_count": 1,
                  "max_reward": 0.8726657734243746,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      242.125,
                      139.834,
                      152.708,
                      173.958,
                      155.875,
                      143.333,
                      147.5,
                      144.542,
                      140.583,
                      144.25,
                      239.833,
                      412.917,
                      234.708,
                      169.209,
                      129.083,
                      137.833,
                      131.333,
                      142.334,
                      135.25,
                      161.708
                    ],
                    "median_us": 147.5,
                    "reward": 0.8726657734243746,
                    "speedup": 0.9813559322033898
                  },
                  "children": []
                },
                {
                  "node_id": "gen5_d3b2676b63d8",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen5",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint CHUNK = 16;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Accumulate sum-of-squares with large chunks\nfloat sumsq = 0.0f;\nfor (uint block = 0; block < D / CHUNK; block++) {\n    uint offset = block * CHUNK;\n    if (offset + tid < D) {\n        float acc0 = 0.0f, acc1 = 0.0f;\n        for (uint i = 0; i < CHUNK && offset + tid + i*TG < D; i += 2) {\n            float v0 = (float)inp[base + offset + tid + i*TG];\n            float v1 = (i+1 < CHUNK && offset + tid + (i+1)*TG < D) ? (float)inp[base + offset + tid + (i+1)*TG] : 0.0f;\n            acc0 += v0 * v0;\n            acc1 += v1 * v1;\n        }\n        sumsq += acc0 + acc1;\n    }\n}\n\n// Single-barrier SIMD reduction\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat total = 0.0f;\nif (tid < 8) {\n    total = simd_sum(buf[tid]);\n}\nfloat inv = metal::rsqrt(total / (float)D + EPS);\n\n// Write output\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_adb8f750d639",
                    "generation": 5,
                    "llm_reasoning": "Reduce synchronization overhead by using only SIMD reductions with a single threadgroup barrier, and process data in larger chunks (16 elements per thread per iteration) to improve memory throughput."
                  },
                  "visit_count": 1,
                  "max_reward": 0.0,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": false,
                    "compile_error": null,
                    "correctness_error": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\nOutput 0 mismatch\nMismatched elements: 2048 / 2048 (100%)\nFirst 5 mismatches are at indices:\n [0, 0]: -0.3535471558570862 (ACTUAL), -2.82169508934021 (DESIRED)\n [0, 1]: -0.001253017457202077 (ACTUAL), -0.010000457055866718 (DESIRED)\n [0, 2]: -0.06565272808074951 (ACTUAL), -0.523980975151062 (DESIRED)\n [0, 3]: -0.002824117662385106 (ACTUAL), -0.022539563477039337 (DESIRED)\n [0, 4]: -0.18274591863155365 (ACTUAL), -1.4585134983062744 (DESIRED)\nMax absolute difference among violations: 8176.4014\nMax relative difference among violations: 1003.0089\n ACTUAL: array([[-3.535472e-01, -1.253017e-03, -6.565273e-02, ..., -2.786683e+03,\n        -4.381265e+02,  7.333153e+02]], shape=(1, 2048), dtype=float32)\n DESIRED: array([[-2.821695, -0.01    , -0.523981, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)",
                    "timings_us": [],
                    "median_us": Infinity,
                    "reward": 0.0,
                    "speedup": 0.0
                  },
                  "children": []
                },
                {
                  "node_id": "gen5_5e5134603ba3",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen5",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\nfloat cached[8];  // Cache input values in registers\n\n// Load and accumulate in same pass\nfloat sumsq = 0.0f;\nuint cache_idx = 0;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    if (cache_idx < 8) {\n        cached[cache_idx++] = v;\n    }\n    sumsq += v * v;\n}\n\n// Two-stage reduction\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Write cached values first (no load needed)\ncache_idx = 0;\nfor (uint j = tid; j < D && cache_idx < 8; j += TG, cache_idx++) {\n    out[base + j] = (T)(cached[cache_idx] * inv * (float)weight[j]);\n}\n\n// Handle remaining elements\nfor (uint j = tid + 8*TG; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_adb8f750d639",
                    "generation": 5,
                    "llm_reasoning": "Maximize register reuse by computing partial normalized outputs during the reduction phase itself, storing only partial sums in shared memory, eliminating the need to re-read input data."
                  },
                  "visit_count": 1,
                  "max_reward": 0.9665419246364743,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      163.584,
                      165.125,
                      143.042,
                      139.416,
                      164.917,
                      122.375,
                      172.417,
                      132.0,
                      126.334,
                      162.75,
                      180.75,
                      183.084,
                      155.417,
                      184.875,
                      143.666,
                      136.625,
                      133.458,
                      131.083,
                      127.5,
                      152.084
                    ],
                    "median_us": 152.084,
                    "reward": 0.9665419246364743,
                    "speedup": 0.9517766497461929
                  },
                  "children": []
                },
                {
                  "node_id": "gen5_bcacf285d98a",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen5",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Process with SIMD-aligned strides (32 elements at a time)\nfloat sumsq = 0.0f;\nfor (uint block = simd_group; block < D/32; block += 8) {\n    uint idx = block * 32 + simd_lane;\n    float v = (float)inp[base + idx];\n    sumsq += v * v;\n}\n\n// Handle remainder if D not multiple of 256\nif (simd_group == 7) {\n    for (uint idx = 224 + simd_lane; idx < D; idx += 32) {\n        float v = (float)inp[base + idx];\n        sumsq += v * v;\n    }\n}\n\n// SIMD reduction within each warp\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final cross-warp reduction\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Write output with same SIMD-aligned pattern\nfor (uint block = simd_group; block < D/32; block += 8) {\n    uint idx = block * 32 + simd_lane;\n    out[base + idx] = (T)((float)inp[base + idx] * inv * (float)weight[idx]);\n}\nif (simd_group == 7) {\n    for (uint idx = 224 + simd_lane; idx < D; idx += 32) {\n        out[base + idx] = (T)((float)inp[base + idx] * inv * (float)weight[idx]);\n    }\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_adb8f750d639",
                    "generation": 5,
                    "llm_reasoning": "Optimize for Apple Silicon's wide SIMD by processing 32 elements per threadgroup iteration (matching SIMD width), with each SIMD group handling contiguous memory regions to maximize coalescing and minimize warp divergence."
                  },
                  "visit_count": 1,
                  "max_reward": 0.0,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": false,
                    "compile_error": null,
                    "correctness_error": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\nOutput 0 mismatch\nMismatched elements: 2048 / 2048 (100%)\nFirst 5 mismatches are at indices:\n [0, 0]: -2.059082508087158 (ACTUAL), -2.82169508934021 (DESIRED)\n [0, 1]: -0.0072976588271558285 (ACTUAL), -0.010000457055866718 (DESIRED)\n [0, 2]: -0.3823659121990204 (ACTUAL), -0.523980975151062 (DESIRED)\n [0, 3]: -0.016447853296995163 (ACTUAL), -0.022539563477039337 (DESIRED)\n [0, 4]: -1.0643246173858643 (ACTUAL), -1.4585134983062744 (DESIRED)\nMax absolute difference among violations: 2.203187\nMax relative difference among violations: 0.27026764\n ACTUAL: array([[-2.059083, -0.007298, -0.382366, ..., -2.025414, -0.318439,\n         0.532987]], shape=(1, 2048), dtype=float32)\n DESIRED: array([[-2.821695, -0.01    , -0.523981, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)",
                    "timings_us": [],
                    "median_us": Infinity,
                    "reward": 0.0,
                    "speedup": 0.0
                  },
                  "children": []
                }
              ]
            },
            {
              "node_id": "gen2_ccd8740b6477",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen2",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat w_cache[8];\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        w_cache[k] = (float)weight[idx];\n    }\n}\n\nfloat sumsq = 0.0f;\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        sumsq += v * v;\n    }\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)((float)inp[base + idx] * inv * w_cache[k]);\n    }\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_8d1551be6855",
                "generation": 2,
                "llm_reasoning": "Prefetch weight values into registers during the first loop to hide memory latency, then reuse in the second loop to reduce global memory traffic."
              },
              "visit_count": 5,
              "max_reward": 1.126203824657362,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": true,
                "compile_error": null,
                "correctness_error": null,
                "timings_us": [
                  152.667,
                  153.208,
                  124.208,
                  122.916,
                  119.125,
                  119.5,
                  117.0,
                  180.916,
                  192.708,
                  134.291,
                  144.334,
                  110.125,
                  161.208,
                  168.375,
                  124.916,
                  127.625,
                  141.166,
                  123.916,
                  146.958,
                  126.583
                ],
                "median_us": 134.291,
                "reward": 1.0492654955759106,
                "speedup": 1.0778831046012018
              },
              "children": [
                {
                  "node_id": "gen4_aa150a4a2bc7",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen4",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG + 8]; // padding to reduce bank conflicts\n\nfloat w_cache[8];\nfloat x_cache[8];\nfloat sumsq = 0.0f;\n\n#pragma unroll\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        float w = (float)weight[idx];\n        w_cache[k] = w;\n        x_cache[k] = v;\n        sumsq += v * v;\n    }\n}\n\nbuf[tid] = sumsq;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 128) buf[tid] += buf[tid + 128];\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nif (tid < 64) buf[tid] += buf[tid + 64];\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nif (tid < 32) buf[tid] += buf[tid + 32];\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 32) {\n    float val = buf[tid] + buf[tid + 16];\n    val += metal::simd_shuffle_down(val, 8);\n    val += metal::simd_shuffle_down(val, 4);\n    val += metal::simd_shuffle_down(val, 2);\n    val += metal::simd_shuffle_down(val, 1);\n    if (tid == 0) buf[0] = val;\n}\n\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n#pragma unroll\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_ccd8740b6477",
                    "generation": 4,
                    "llm_reasoning": "Bank conflict reduction by using strided access pattern with padding in shared memory, plus register blocking to hide latency during final output write."
                  },
                  "visit_count": 1,
                  "max_reward": 0.9547253024956687,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      141.583,
                      151.375,
                      256.875,
                      192.25,
                      189.584,
                      145.375,
                      128.0,
                      131.583,
                      141.25,
                      134.125,
                      140.292,
                      148.417,
                      189.583,
                      148.292,
                      136.416,
                      131.083,
                      132.084,
                      179.041,
                      136.208,
                      131.25
                    ],
                    "median_us": 141.583,
                    "reward": 0.9547253024956687,
                    "speedup": 1.0223685046933602
                  },
                  "children": []
                },
                {
                  "node_id": "gen4_bda0aae130b0",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen4",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[32]; // only need per-warp storage\n\nfloat w_cache[8];\nfloat x_cache[8];\nfloat sumsq = 0.0f;\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        float w = (float)weight[idx];\n        w_cache[k] = w;\n        x_cache[k] = v;\n        sumsq += v * v;\n    }\n}\n\n// SIMD-level reduction first\nfloat simd_sum_val = metal::simd_sum(sumsq);\n\nuint warp_id = tid / 32;\nuint lane_id = tid % 32;\n\nif (lane_id == 0) {\n    buf[warp_id] = simd_sum_val;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction across warps\nfloat total = 0.0f;\nif (tid < 32) {\n    total = (tid < 8) ? buf[tid] : 0.0f;\n    total = metal::simd_sum(total);\n    if (tid == 0) buf[0] = total;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_ccd8740b6477",
                    "generation": 4,
                    "llm_reasoning": "Hybrid warp-level + threadgroup reduction with early computation start - compute simd_sum first to reduce threadgroup barrier pressure, then use smaller shared buffer."
                  },
                  "visit_count": 1,
                  "max_reward": 0.9717724463853379,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      209.625,
                      154.666,
                      148.583,
                      184.083,
                      199.208,
                      142.625,
                      156.25,
                      177.25,
                      145.625,
                      143.292,
                      122.25,
                      123.875,
                      123.834,
                      164.584,
                      141.25,
                      167.958,
                      163.292,
                      92.916,
                      130.417,
                      137.042
                    ],
                    "median_us": 148.583,
                    "reward": 0.9717724463853379,
                    "speedup": 0.9742029707301643
                  },
                  "children": []
                },
                {
                  "node_id": "gen4_7f0e47ee7586",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen4",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat w_cache[8];\nfloat x_cache[8];\n\n// Prefetch first batch\nuint idx0 = tid;\nif (idx0 < D) {\n    x_cache[0] = (float)inp[base + idx0];\n    w_cache[0] = (float)weight[idx0];\n}\n\nfloat sumsq = 0.0f;\nfor (uint k = 0; k < 7; k++) {\n    uint idx_next = tid + (k + 1) * TG;\n    \n    // Prefetch next while computing current\n    if (idx_next < D) {\n        x_cache[k + 1] = (float)inp[base + idx_next];\n        w_cache[k + 1] = (float)weight[idx_next];\n    }\n    \n    // Compute current\n    uint idx_curr = tid + k * TG;\n    if (idx_curr < D) {\n        float v = x_cache[k];\n        sumsq += v * v;\n    }\n}\n\n// Last iteration\nuint idx_last = tid + 7 * TG;\nif (idx_last < D) {\n    float v = x_cache[7];\n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_ccd8740b6477",
                    "generation": 4,
                    "llm_reasoning": "Double buffering with software pipelining - overlap next iteration's loads with current iteration's computation to maximize memory bandwidth utilization."
                  },
                  "visit_count": 1,
                  "max_reward": 1.126203824657362,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      131.417,
                      125.542,
                      123.584,
                      126.875,
                      143.25,
                      245.458,
                      123.125,
                      124.084,
                      124.666,
                      125.125,
                      121.459,
                      120.833,
                      122.375,
                      121.333,
                      121.792,
                      119.583,
                      121.833,
                      121.416,
                      122.541,
                      121.375
                    ],
                    "median_us": 123.125,
                    "reward": 1.126203824657362,
                    "speedup": 1.1756345177664975
                  },
                  "children": []
                },
                {
                  "node_id": "gen4_3687624aa661",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen4",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float atomic_sum;\nthreadgroup float buf[TG];\n\nif (tid == 0) atomic_sum = 0.0f;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat w_cache[8];\nfloat x_cache[8];\nfloat sumsq = 0.0f;\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        float w = (float)weight[idx];\n        w_cache[k] = w;\n        x_cache[k] = v;\n        sumsq += v * v;\n    }\n}\n\n// Warp-level reduction only\nfloat warp_sum = metal::simd_sum(sumsq);\nuint lane = tid % 32;\n\nif (lane == 0) {\n    buf[tid / 32] = warp_sum;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Single warp does final reduction\nfloat total = 0.0f;\nif (tid < 8) {\n    total = buf[tid] + metal::simd_shuffle_down(buf[tid], 4);\n    total += metal::simd_shuffle_down(total, 2);\n    total += metal::simd_shuffle_down(total, 1);\n    if (tid == 0) atomic_sum = total;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(atomic_sum / (float)D + EPS);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_ccd8740b6477",
                    "generation": 4,
                    "llm_reasoning": "Minimize threadgroup barriers by using warp-level primitives exclusively and computing partial results independently, then use atomic operations for final reduction."
                  },
                  "visit_count": 1,
                  "max_reward": 0.9600815111434806,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      123.583,
                      124.292,
                      127.083,
                      123.166,
                      126.5,
                      123.792,
                      123.375,
                      129.541,
                      145.792,
                      179.75,
                      410.291,
                      240.541,
                      174.833,
                      167.0,
                      151.625,
                      147.333,
                      159.0,
                      132.666,
                      129.292,
                      133.042
                    ],
                    "median_us": 133.042,
                    "reward": 0.9600815111434806,
                    "speedup": 1.0880022849927091
                  },
                  "children": []
                }
              ]
            },
            {
              "node_id": "gen2_b28fbd535738",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen2",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 512;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v0 = (float)inp[base + j];\n    float v1 = (j + TG < D) ? (float)inp[base + j + TG] : 0.0f;\n    float v2 = (j + TG*2 < D) ? (float)inp[base + j + TG*2] : 0.0f;\n    float v3 = (j + TG*3 < D) ? (float)inp[base + j + TG*3] : 0.0f;\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j = tid; j < D; j += TG) {\n    if (j < D) out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    if (j + TG < D) out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * (float)weight[j + TG]);\n    if (j + TG*2 < D) out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * (float)weight[j + TG*2]);\n    if (j + TG*3 < D) out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * (float)weight[j + TG*3]);\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_8d1551be6855",
                "generation": 2,
                "llm_reasoning": "Increase threadgroup size to 512 to double occupancy and reduce number of synchronization points, with 4x unroll factor optimized for the new threadgroup size."
              },
              "visit_count": 1,
              "max_reward": 0.0,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": false,
                "compile_error": null,
                "correctness_error": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\nOutput 0 mismatch\nMismatched elements: 1017 / 2048 (49.7%)\nFirst 5 mismatches are at indices:\n [0, 0]: -2.6251142024993896 (ACTUAL), -2.82169508934021 (DESIRED)\n [0, 1]: -0.009303749538958073 (ACTUAL), -0.010000457055866718 (DESIRED)\n [0, 2]: -0.4874764382839203 (ACTUAL), -0.523980975151062 (DESIRED)\n [0, 3]: -0.020969286561012268 (ACTUAL), -0.022539563477039337 (DESIRED)\n [0, 4]: -1.3569023609161377 (ACTUAL), -1.4585134983062744 (DESIRED)\nMax absolute difference among violations: 0.37542582\nMax relative difference among violations: 0.06966776\n ACTUAL: array([[-2.625114, -0.009304, -0.487476, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)\n DESIRED: array([[-2.821695, -0.01    , -0.523981, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)",
                "timings_us": [],
                "median_us": Infinity,
                "reward": 0.0,
                "speedup": 0.0
              },
              "children": []
            },
            {
              "node_id": "gen2_f6cd826132fb",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen2",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float sumsq_buf[TG];\nthreadgroup float val_buf[D];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    val_buf[j] = v;\n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(sumsq_buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(sumsq_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)(val_buf[j] * inv * (float)weight[j]);\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_8d1551be6855",
                "generation": 2,
                "llm_reasoning": "Completely fuse both phases into a single loop by computing partial output values during reduction phase, storing them in threadgroup memory to eliminate the second global memory read pass."
              },
              "visit_count": 9,
              "max_reward": 1.1495834890142032,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": true,
                "compile_error": null,
                "correctness_error": null,
                "timings_us": [
                  165.5,
                  153.417,
                  155.583,
                  172.958,
                  159.792,
                  150.083,
                  185.667,
                  473.167,
                  239.333,
                  170.834,
                  120.958,
                  198.875,
                  154.375,
                  143.625,
                  147.125,
                  139.875,
                  139.916,
                  139.333,
                  139.5,
                  123.166
                ],
                "median_us": 154.375,
                "reward": 0.8787341183304566,
                "speedup": 0.9376518218623482
              },
              "children": [
                {
                  "node_id": "gen9_5563d1868dd8",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen9",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint CHUNK = 16;  // 64-byte cache line\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float sumsq_buf[TG];\nthreadgroup float val_buf[D];\n\n// Load and reduce in cache-line-sized chunks\nfloat sumsq = 0.0f;\nfor (uint chunk_base = 0; chunk_base < D; chunk_base += TG * CHUNK) {\n    uint idx = chunk_base + tid * CHUNK;\n    if (idx + CHUNK <= D) {\n        // Aligned 64-byte chunk read\n        for (uint k = 0; k < CHUNK; k++) {\n            float v = (float)inp[base + idx + k];\n            val_buf[idx + k] = v;\n            sumsq += v * v;\n        }\n    } else {\n        // Handle remainder\n        for (uint k = 0; k < CHUNK && idx + k < D; k++) {\n            float v = (float)inp[base + idx + k];\n            val_buf[idx + k] = v;\n            sumsq += v * v;\n        }\n    }\n}\n\nKK_SIMD_REDUCE_SUM(sumsq_buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(sumsq_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Write in cache-line chunks\nfor (uint chunk_base = 0; chunk_base < D; chunk_base += TG * CHUNK) {\n    uint idx = chunk_base + tid * CHUNK;\n    if (idx + CHUNK <= D) {\n        for (uint k = 0; k < CHUNK; k++) {\n            out[base + idx + k] = (T)(val_buf[idx + k] * inv * (float)weight[idx + k]);\n        }\n    } else {\n        for (uint k = 0; k < CHUNK && idx + k < D; k++) {\n            out[base + idx + k] = (T)(val_buf[idx + k] * inv * (float)weight[idx + k]);\n        }\n    }\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_f6cd826132fb",
                    "generation": 9,
                    "llm_reasoning": "Explicitly chunk loads to 64-byte cache lines (16 floats) to match M4's cache subsystem, maximizing spatial locality and prefetcher effectiveness."
                  },
                  "visit_count": 1,
                  "max_reward": 0.9686198960831927,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      179.625,
                      170.792,
                      151.209,
                      179.542,
                      158.25,
                      153.167,
                      150.583,
                      148.667,
                      152.167,
                      179.167,
                      148.417,
                      134.458,
                      122.166,
                      135.833,
                      135.875,
                      132.333,
                      131.334,
                      126.125,
                      155.916,
                      162.834
                    ],
                    "median_us": 151.209,
                    "reward": 0.9686198960831927,
                    "speedup": 0.957284288633613
                  },
                  "children": []
                },
                {
                  "node_id": "gen9_c1b180269a14",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen9",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint lane_id = tid & 31;\nuint warp_id = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float warp_sums[8];  // 8 warps in TG=256\nthreadgroup float val_buf[D];\n\n// Phase 1: Load and compute per-thread sum\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    val_buf[j] = v;\n    sumsq += v * v;\n}\n\n// Phase 2: SIMD-level reduction (32 threads per warp)\nfor (uint offset = 16; offset > 0; offset >>= 1) {\n    sumsq += simd_shuffle_down(sumsq, offset);\n}\n\n// Phase 3: Warp leaders write to shared memory\nif (lane_id == 0) {\n    warp_sums[warp_id] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 4: Final reduction by first warp\nfloat total_sumsq = 0.0f;\nif (tid < 8) {\n    total_sumsq = warp_sums[tid];\n    for (uint offset = 4; offset > 0; offset >>= 1) {\n        total_sumsq += simd_shuffle_down(total_sumsq, offset);\n    }\n}\n\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nfloat inv = metal::rsqrt(total_sumsq / (float)D + EPS);\n\n// Phase 5: Compute output\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)(val_buf[j] * inv * (float)weight[j]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_f6cd826132fb",
                    "generation": 9,
                    "llm_reasoning": "Use explicit SIMD lane cooperation with simd_shuffle to compute partial sums across 32-wide lanes before threadgroup reduction, reducing shared memory traffic."
                  },
                  "visit_count": 1,
                  "max_reward": 0.0,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": false,
                    "compile_error": null,
                    "correctness_error": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\nOutput 0 mismatch\nMismatched elements: 2040 / 2048 (99.6%)\nFirst 5 mismatches are at indices:\n [0, 1]: -0.010824729688465595 (ACTUAL), -0.010000457055866718 (DESIRED)\n [0, 2]: -0.6138868927955627 (ACTUAL), -0.523980975151062 (DESIRED)\n [0, 3]: -0.02887003868818283 (ACTUAL), -0.022539563477039337 (DESIRED)\n [0, 4]: -2.0897645950317383 (ACTUAL), -1.4585134983062744 (DESIRED)\n [0, 5]: -0.35150501132011414 (ACTUAL), -0.21335050463676453 (DESIRED)\nMax absolute difference among violations: 8176.4014\nMax relative difference among violations: 1003.0089\n ACTUAL: array([[-2.821695e+00, -1.082473e-02, -6.138869e-01, ..., -2.786683e+03,\n        -4.381265e+02,  7.333153e+02]], shape=(1, 2048), dtype=float32)\n DESIRED: array([[-2.821695, -0.01    , -0.523981, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)",
                    "timings_us": [],
                    "median_us": Infinity,
                    "reward": 0.0,
                    "speedup": 0.0
                  },
                  "children": []
                },
                {
                  "node_id": "gen9_f81f90f5a6a3",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen9",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;  // 8\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float final_sum[1];\nfloat local_vals[ELEMS_PER_THREAD];\nfloat local_weights[ELEMS_PER_THREAD];\n\n// Level 1: Register accumulation (8 elements per thread)\nfloat sumsq = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    local_vals[k] = (float)inp[base + offset + k];\n    local_weights[k] = (float)weight[offset + k];\n    sumsq += local_vals[k] * local_vals[k];\n}\n\n// Level 2: SIMD reduction (32 lanes)\nfor (uint s = 16; s > 0; s >>= 1) {\n    sumsq += simd_shuffle_down(sumsq, s);\n}\n\n// Level 3: Threadgroup reduction (8 SIMD groups)\nthreadgroup float simd_results[TG / 32];\nuint lane = tid & 31;\nuint simd_idx = tid >> 5;\n\nif (lane == 0) {\n    simd_results[simd_idx] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid == 0) {\n    float total = 0.0f;\n    for (uint i = 0; i < TG / 32; i++) {\n        total += simd_results[i];\n    }\n    final_sum[0] = total;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(final_sum[0] / (float)D + EPS);\n\n// Direct register output\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(local_vals[k] * inv * local_weights[k]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_f6cd826132fb",
                    "generation": 9,
                    "llm_reasoning": "Three-level reduction hierarchy (register \u2192 SIMD \u2192 threadgroup) with minimal barriers and direct register-to-register computation for lowest latency."
                  },
                  "visit_count": 5,
                  "max_reward": 1.1495834890142032,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      129.0,
                      126.542,
                      152.041,
                      155.625,
                      144.0,
                      131.709,
                      133.875,
                      123.75,
                      125.291,
                      124.833,
                      124.041,
                      123.167,
                      115.583,
                      113.833,
                      113.375,
                      113.5,
                      114.208,
                      115.0,
                      126.916,
                      122.625
                    ],
                    "median_us": 124.833,
                    "reward": 1.1495834890142032,
                    "speedup": 1.1595491576746533
                  },
                  "children": [
                    {
                      "node_id": "gen10_fa94523d84f7",
                      "candidate": {
                        "spec": {
                          "name": "kk_glm_rmsnorm_gen10",
                          "input_names": [
                            "inp",
                            "weight"
                          ],
                          "output_names": [
                            "out"
                          ],
                          "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float simd_partials[8];\nfloat local_vals[ELEMS_PER_THREAD];\nfloat local_weights[ELEMS_PER_THREAD];\n\n// Register accumulation\nfloat sumsq = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    local_vals[k] = (float)inp[base + offset + k];\n    local_weights[k] = (float)weight[offset + k];\n    sumsq += local_vals[k] * local_vals[k];\n}\n\n// SIMD reduction\nsumsq += simd_shuffle_down(sumsq, 16);\nsumsq += simd_shuffle_down(sumsq, 8);\nsumsq += simd_shuffle_down(sumsq, 4);\nsumsq += simd_shuffle_down(sumsq, 2);\nsumsq += simd_shuffle_down(sumsq, 1);\n\nuint lane = tid & 31;\nuint simd_idx = tid >> 5;\n\n// Lane 0 of each warp writes partial\nif (lane == 0) {\n    simd_partials[simd_idx] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// First warp does final reduction and broadcasts\nfloat inv;\nif (tid < 8) {\n    float total = simd_partials[tid];\n    total += simd_shuffle_down(total, 4);\n    total += simd_shuffle_down(total, 2);\n    total += simd_shuffle_down(total, 1);\n    inv = metal::rsqrt(total / (float)D + EPS);\n    simd_partials[0] = inv;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\ninv = simd_partials[0];\n\n// Unrolled output with explicit FMA\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(local_vals[k] * inv * local_weights[k]);\n}",
                          "header": "",
                          "threadgroup": [
                            256,
                            1,
                            1
                          ],
                          "template_params": [
                            [
                              "T",
                              "float32"
                            ]
                          ]
                        },
                        "parent_id": "gen9_f81f90f5a6a3",
                        "generation": 10,
                        "llm_reasoning": "Use warp-specialized roles where first 8 threads handle final reduction while others prepare output values in parallel, eliminating the final barrier."
                      },
                      "visit_count": 1,
                      "max_reward": 0.7922111480379842,
                      "prior": 0.25,
                      "eval_result": {
                        "compiled": true,
                        "correct": true,
                        "compile_error": null,
                        "correctness_error": null,
                        "timings_us": [
                          190.542,
                          212.375,
                          172.417,
                          157.125,
                          184.417,
                          206.625,
                          212.875,
                          188.834,
                          174.0,
                          206.792,
                          194.875,
                          300.958,
                          192.667,
                          193.333,
                          175.084,
                          169.333,
                          148.75,
                          143.583,
                          142.625,
                          144.292
                        ],
                        "median_us": 188.834,
                        "reward": 0.7922111480379842,
                        "speedup": 0.7665462787421756
                      },
                      "children": []
                    },
                    {
                      "node_id": "gen10_ccc9b4566b84",
                      "candidate": {
                        "spec": {
                          "name": "kk_glm_rmsnorm_gen10",
                          "input_names": [
                            "inp",
                            "weight"
                          ],
                          "output_names": [
                            "out"
                          ],
                          "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float shared_inv[1];\nfloat vals[ELEMS_PER_THREAD];\nfloat wts[ELEMS_PER_THREAD];\n\n// Load and accumulate\nfloat sumsq = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\n#pragma unroll\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    vals[k] = (float)inp[base + offset + k];\n    wts[k] = (float)weight[offset + k];\n    sumsq = metal::fma(vals[k], vals[k], sumsq);\n}\n\n// Warp reduction\nfor (uint s = 16; s > 0; s >>= 1) {\n    sumsq += simd_shuffle_down(sumsq, s);\n}\n\n// Threadgroup reduction\nthreadgroup float warp_sums[8];\nuint lane = tid & 31;\nuint warp_id = tid >> 5;\n\nif (lane == 0) warp_sums[warp_id] = sumsq;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid == 0) {\n    float total = warp_sums[0] + warp_sums[1] + warp_sums[2] + warp_sums[3] +\n                  warp_sums[4] + warp_sums[5] + warp_sums[6] + warp_sums[7];\n    shared_inv[0] = metal::rsqrt(total / (float)D + EPS);\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = shared_inv[0];\n\n// Fused multiply-add chain: inv*weight then result*val\n#pragma unroll\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    float scale = inv * wts[k];\n    out[base + offset + k] = (T)(vals[k] * scale);\n}",
                          "header": "",
                          "threadgroup": [
                            256,
                            1,
                            1
                          ],
                          "template_params": [
                            [
                              "T",
                              "float32"
                            ]
                          ]
                        },
                        "parent_id": "gen9_f81f90f5a6a3",
                        "generation": 10,
                        "llm_reasoning": "Exploit FMA units by restructuring computation as fused multiply-add chains, computing inv*weight first then combining with input in single operation."
                      },
                      "visit_count": 1,
                      "max_reward": 1.035774149368919,
                      "prior": 0.25,
                      "eval_result": {
                        "compiled": true,
                        "correct": true,
                        "compile_error": null,
                        "correctness_error": null,
                        "timings_us": [
                          146.75,
                          141.208,
                          144.833,
                          161.959,
                          156.708,
                          139.125,
                          141.542,
                          132.333,
                          131.292,
                          130.125,
                          117.0,
                          131.042,
                          167.209,
                          131.666,
                          126.167,
                          167.375,
                          150.625,
                          153.75,
                          123.708,
                          116.583
                        ],
                        "median_us": 141.208,
                        "reward": 1.035774149368919,
                        "speedup": 1.025083564670557
                      },
                      "children": []
                    },
                    {
                      "node_id": "gen10_524764fd5e17",
                      "candidate": {
                        "spec": {
                          "name": "kk_glm_rmsnorm_gen10",
                          "input_names": [
                            "inp",
                            "weight"
                          ],
                          "output_names": [
                            "out"
                          ],
                          "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduction_buffer[8];\nfloat vals[ELEMS_PER_THREAD];\nfloat weights[ELEMS_PER_THREAD];\n\n// Dual accumulator for odd/even elements\nfloat sumsq_even = 0.0f;\nfloat sumsq_odd = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\n\n// Process pairs to exploit dual-issue\nfor (uint k = 0; k < ELEMS_PER_THREAD; k += 2) {\n    vals[k] = (float)inp[base + offset + k];\n    vals[k+1] = (float)inp[base + offset + k + 1];\n    weights[k] = (float)weight[offset + k];\n    weights[k+1] = (float)weight[offset + k + 1];\n    sumsq_even += vals[k] * vals[k];\n    sumsq_odd += vals[k+1] * vals[k+1];\n}\n\nfloat sumsq = sumsq_even + sumsq_odd;\n\n// Butterfly SIMD reduction\nsumsq += simd_shuffle_xor(sumsq, 16);\nsumsq += simd_shuffle_xor(sumsq, 8);\nsumsq += simd_shuffle_xor(sumsq, 4);\nsumsq += simd_shuffle_xor(sumsq, 2);\nsumsq += simd_shuffle_xor(sumsq, 1);\n\nuint lane = tid & 31;\nuint warp_id = tid >> 5;\n\nif (lane == 0) reduction_buffer[warp_id] = sumsq;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv;\nif (tid == 0) {\n    float total = 0.0f;\n    for (uint i = 0; i < 8; i++) total += reduction_buffer[i];\n    inv = metal::rsqrt(total / (float)D + EPS);\n    reduction_buffer[0] = inv;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\ninv = reduction_buffer[0];\n\n// Pipelined stores with register rotation\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(vals[k] * inv * weights[k]);\n}",
                          "header": "",
                          "threadgroup": [
                            256,
                            1,
                            1
                          ],
                          "template_params": [
                            [
                              "T",
                              "float32"
                            ]
                          ]
                        },
                        "parent_id": "gen9_f81f90f5a6a3",
                        "generation": 10,
                        "llm_reasoning": "Use asymmetric butterfly reduction pattern optimized for M4's dual-issue capabilities, processing odd/even indices separately to maximize port utilization."
                      },
                      "visit_count": 1,
                      "max_reward": 0.6554533093543958,
                      "prior": 0.25,
                      "eval_result": {
                        "compiled": true,
                        "correct": true,
                        "compile_error": null,
                        "correctness_error": null,
                        "timings_us": [
                          177.208,
                          175.625,
                          463.25,
                          528.458,
                          511.541,
                          505.708,
                          524.334,
                          512.584,
                          153.375,
                          153.0,
                          164.208,
                          155.958,
                          149.5,
                          144.375,
                          158.791,
                          150.916,
                          148.583,
                          148.458,
                          147.167,
                          142.417
                        ],
                        "median_us": 158.791,
                        "reward": 0.6554533093543958,
                        "speedup": 0.9115755930751743
                      },
                      "children": []
                    },
                    {
                      "node_id": "gen10_f505fd4fe1c9",
                      "candidate": {
                        "spec": {
                          "name": "kk_glm_rmsnorm_gen10",
                          "input_names": [
                            "inp",
                            "weight"
                          ],
                          "output_names": [
                            "out"
                          ],
                          "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\n// Minimal shared memory - single cache line\nthreadgroup float shared[16];\nfloat regs[ELEMS_PER_THREAD];\nfloat wt_regs[ELEMS_PER_THREAD];\n\n// Accumulate in registers\nfloat acc = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\n\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    regs[k] = (float)inp[base + offset + k];\n    wt_regs[k] = (float)weight[offset + k];\n    acc += regs[k] * regs[k];\n}\n\n// Full SIMD reduction (32\u21921)\nacc += simd_shuffle_down(acc, 16);\nacc += simd_shuffle_down(acc, 8);\nacc += simd_shuffle_down(acc, 4);\nacc += simd_shuffle_down(acc, 2);\nacc += simd_shuffle_down(acc, 1);\n\nuint lane = tid & 31;\nuint warp = tid >> 5;\n\n// Only 8 warps write\nif (lane == 0 && warp < 8) {\n    shared[warp] = acc;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Single thread final reduction\nif (tid == 0) {\n    float sum = shared[0] + shared[1] + shared[2] + shared[3] +\n                shared[4] + shared[5] + shared[6] + shared[7];\n    shared[0] = metal::rsqrt(sum / (float)D + EPS);\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat scale = shared[0];\n\n// Compute and store with compiler-friendly loop\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(regs[k] * scale * wt_regs[k]);\n}",
                          "header": "",
                          "threadgroup": [
                            256,
                            1,
                            1
                          ],
                          "template_params": [
                            [
                              "T",
                              "float32"
                            ]
                          ]
                        },
                        "parent_id": "gen9_f81f90f5a6a3",
                        "generation": 10,
                        "llm_reasoning": "Minimize threadgroup memory footprint to single cache line (64B = 16 floats) and use wider SIMD tree with explicit lane masking for final stages."
                      },
                      "visit_count": 1,
                      "max_reward": 0.8268625240155393,
                      "prior": 0.25,
                      "eval_result": {
                        "compiled": true,
                        "correct": true,
                        "compile_error": null,
                        "correctness_error": null,
                        "timings_us": [
                          193.375,
                          207.375,
                          204.667,
                          180.0,
                          169.208,
                          165.292,
                          183.25,
                          165.417,
                          178.375,
                          174.25,
                          157.75,
                          164.958,
                          155.875,
                          147.625,
                          263.292,
                          170.042,
                          179.667,
                          160.958,
                          167.292,
                          144.709
                        ],
                        "median_us": 170.042,
                        "reward": 0.8268625240155393,
                        "speedup": 0.8512602768727726
                      },
                      "children": []
                    }
                  ]
                },
                {
                  "node_id": "gen9_d505e8fc5f94",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen9",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float sumsq_buf[TG];\nthreadgroup float val_buf[D];\nfloat weight_cache[8];  // Register cache for weights\n\n// Fused load: input values, weights, and compute sumsq\nfloat sumsq = 0.0f;\nuint elem_count = 0;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    float w = (float)weight[j];\n    \n    val_buf[j] = v;\n    \n    // Cache weight in registers for reuse\n    if (elem_count < 8) {\n        weight_cache[elem_count] = w;\n    }\n    elem_count++;\n    \n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(sumsq_buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(sumsq_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Output phase: use cached weights when available\nelem_count = 0;\nfor (uint j = tid; j < D; j += TG) {\n    float w = (elem_count < 8) ? weight_cache[elem_count] : (float)weight[j];\n    out[base + j] = (T)(val_buf[j] * inv * w);\n    elem_count++;\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen2_f6cd826132fb",
                    "generation": 9,
                    "llm_reasoning": "Fused load-multiply-accumulate with weight prefetching into registers during reduction phase, eliminating second global memory pass entirely."
                  },
                  "visit_count": 1,
                  "max_reward": 1.1072405235542588,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      130.208,
                      121.792,
                      135.333,
                      140.291,
                      122.875,
                      120.083,
                      120.166,
                      124.167,
                      122.25,
                      179.834,
                      122.25,
                      119.417,
                      119.458,
                      120.084,
                      209.417,
                      139.375,
                      121.25,
                      128.5,
                      125.0,
                      122.541
                    ],
                    "median_us": 122.875,
                    "reward": 1.1072405235542588,
                    "speedup": 1.1780264496439472
                  },
                  "children": []
                }
              ]
            }
          ]
        },
        {
          "node_id": "gen1_503ee400fc28",
          "candidate": {
            "spec": {
              "name": "kk_glm_rmsnorm_gen1",
              "input_names": [
                "inp",
                "weight"
              ],
              "output_names": [
                "out"
              ],
              "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Compute sum-of-squares with 4x unroll\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Fused multiply with register reuse\nfor (uint j = tid; j < D; j += TG * 4) {\n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    out[base + j] = (T)((float)inp[base + j] * inv * w0);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * w1);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * w2);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * w3);\n}",
              "header": "",
              "threadgroup": [
                256,
                1,
                1
              ],
              "template_params": [
                [
                  "T",
                  "float32"
                ]
              ]
            },
            "parent_id": "root",
            "generation": 1,
            "llm_reasoning": "Fused computation storing intermediate values in registers to minimize memory traffic, with 4x unroll for ILP."
          },
          "visit_count": 17,
          "max_reward": 1.1405943528570834,
          "prior": 0.25,
          "eval_result": {
            "compiled": true,
            "correct": true,
            "compile_error": null,
            "correctness_error": null,
            "timings_us": [
              455.083,
              237.334,
              153.5,
              133.75,
              127.292,
              122.75,
              125.583,
              128.959,
              126.125,
              129.125,
              133.958,
              132.959,
              162.208,
              197.167,
              213.542,
              136.209,
              218.708,
              138.334,
              130.125,
              129.083
            ],
            "median_us": 133.958,
            "reward": 0.9253581057386301,
            "speedup": 1.0805625643858523
          },
          "children": [
            {
              "node_id": "gen3_1b04b6c690c7",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen3",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Compute sum-of-squares with manual software pipelining\nfloat sumsq = 0.0f;\nfloat prefetch_v0, prefetch_v1, prefetch_v2, prefetch_v3;\n\n// Prefetch first iteration\nif (tid < D) {\n    prefetch_v0 = (float)inp[base + tid];\n    prefetch_v1 = (float)inp[base + tid + TG];\n    prefetch_v2 = (float)inp[base + tid + TG*2];\n    prefetch_v3 = (float)inp[base + tid + TG*3];\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    // Compute with prefetched values\n    float v0 = prefetch_v0;\n    float v1 = prefetch_v1;\n    float v2 = prefetch_v2;\n    float v3 = prefetch_v3;\n    \n    // Prefetch next iteration while computing current\n    uint next_j = j + TG * 4;\n    if (next_j < D) {\n        prefetch_v0 = (float)inp[base + next_j];\n        prefetch_v1 = (float)inp[base + next_j + TG];\n        prefetch_v2 = (float)inp[base + next_j + TG*2];\n        prefetch_v3 = (float)inp[base + next_j + TG*3];\n    }\n    \n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Fused multiply with same pipelining strategy\nif (tid < D) {\n    prefetch_v0 = (float)inp[base + tid];\n    prefetch_v1 = (float)weight[tid];\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    float in0 = prefetch_v0;\n    float w0 = prefetch_v1;\n    \n    uint idx1 = j + TG;\n    uint idx2 = j + TG*2;\n    uint idx3 = j + TG*3;\n    \n    float in1 = (float)inp[base + idx1];\n    float w1 = (float)weight[idx1];\n    float in2 = (float)inp[base + idx2];\n    float w2 = (float)weight[idx2];\n    float in3 = (float)inp[base + idx3];\n    float w3 = (float)weight[idx3];\n    \n    // Prefetch next iteration\n    uint next_j = j + TG * 4;\n    if (next_j < D) {\n        prefetch_v0 = (float)inp[base + next_j];\n        prefetch_v1 = (float)weight[next_j];\n    }\n    \n    out[base + j] = (T)(in0 * inv * w0);\n    out[base + idx1] = (T)(in1 * inv * w1);\n    out[base + idx2] = (T)(in2 * inv * w2);\n    out[base + idx3] = (T)(in3 * inv * w3);\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_503ee400fc28",
                "generation": 3,
                "llm_reasoning": "Split reduction phase from computation phase with pipeline parallelism - prefetch next row's data while computing current row to hide memory latency and improve throughput."
              },
              "visit_count": 1,
              "max_reward": 0.9719511606678779,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": true,
                "compile_error": null,
                "correctness_error": null,
                "timings_us": [
                  167.833,
                  139.625,
                  136.084,
                  165.25,
                  147.625,
                  151.833,
                  143.875,
                  148.875,
                  145.625,
                  145.917,
                  153.5,
                  160.583,
                  151.625,
                  150.583,
                  143.458,
                  148.0,
                  208.0,
                  131.5,
                  127.333,
                  128.75
                ],
                "median_us": 148.0,
                "reward": 0.9719511606678779,
                "speedup": 0.9780405405405406
              },
              "children": []
            },
            {
              "node_id": "gen3_09fc2b3dc2ba",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen3",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Each thread processes 8 elements per iteration in registers\nfloat sumsq = 0.0f;\nuint stride = TG * 8;\n\nfor (uint j = tid; j < D; j += stride) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    uint idx4 = idx0 + TG*4;\n    uint idx5 = idx0 + TG*5;\n    uint idx6 = idx0 + TG*6;\n    uint idx7 = idx0 + TG*7;\n    \n    float v0 = (float)inp[idx0];\n    float v1 = (float)inp[idx1];\n    float v2 = (float)inp[idx2];\n    float v3 = (float)inp[idx3];\n    float v4 = (float)inp[idx4];\n    float v5 = (float)inp[idx5];\n    float v6 = (float)inp[idx6];\n    float v7 = (float)inp[idx7];\n    \n    // Accumulate in pairs to maximize FMA usage\n    float sum01 = v0*v0 + v1*v1;\n    float sum23 = v2*v2 + v3*v3;\n    float sum45 = v4*v4 + v5*v5;\n    float sum67 = v6*v6 + v7*v7;\n    \n    sumsq += (sum01 + sum23) + (sum45 + sum67);\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Same 8-element blocking for output\nfor (uint j = tid; j < D; j += stride) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    uint idx4 = idx0 + TG*4;\n    uint idx5 = idx0 + TG*5;\n    uint idx6 = idx0 + TG*6;\n    uint idx7 = idx0 + TG*7;\n    \n    out[idx0] = (T)((float)inp[idx0] * inv * (float)weight[j]);\n    out[idx1] = (T)((float)inp[idx1] * inv * (float)weight[j + TG]);\n    out[idx2] = (T)((float)inp[idx2] * inv * (float)weight[j + TG*2]);\n    out[idx3] = (T)((float)inp[idx3] * inv * (float)weight[j + TG*3]);\n    out[idx4] = (T)((float)inp[idx4] * inv * (float)weight[j + TG*4]);\n    out[idx5] = (T)((float)inp[idx5] * inv * (float)weight[j + TG*5]);\n    out[idx6] = (T)((float)inp[idx6] * inv * (float)weight[j + TG*6]);\n    out[idx7] = (T)((float)inp[idx7] * inv * (float)weight[j + TG*7]);\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_503ee400fc28",
                "generation": 3,
                "llm_reasoning": "Use aggressive manual register blocking with 8-element chunks per thread, computing partial sums in registers before threadgroup reduction to maximize arithmetic intensity and minimize shared memory pressure."
              },
              "visit_count": 5,
              "max_reward": 1.1405943528570834,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": true,
                "compile_error": null,
                "correctness_error": null,
                "timings_us": [
                  158.375,
                  136.083,
                  128.0,
                  193.167,
                  547.875,
                  234.875,
                  156.292,
                  127.333,
                  126.875,
                  121.708,
                  117.416,
                  131.583,
                  141.209,
                  137.125,
                  136.958,
                  137.125,
                  123.458,
                  125.084,
                  123.583,
                  120.792
                ],
                "median_us": 136.083,
                "reward": 0.9745765145500392,
                "speedup": 1.0636890721104033
              },
              "children": [
                {
                  "node_id": "gen8_0289c06b333b",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen8",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: float4 vectorized reduction (D=2048, 4 elements/load, TG=256 \u2192 2 iterations)\nfloat sumsq = 0.0f;\nuint vec_stride = TG * 4;\n\nfor (uint j = tid * 4; j < D; j += vec_stride) {\n    uint idx = base + j;\n    float4 v = float4(inp[idx], inp[idx+1], inp[idx+2], inp[idx+3]);\n    sumsq += dot(v, v);\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: float4 vectorized output\nfor (uint j = tid * 4; j < D; j += vec_stride) {\n    uint idx = base + j;\n    uint w_idx = j;\n    \n    float4 in_vec = float4(inp[idx], inp[idx+1], inp[idx+2], inp[idx+3]);\n    float4 w_vec = float4(weight[w_idx], weight[w_idx+1], weight[w_idx+2], weight[w_idx+3]);\n    float4 result = in_vec * inv * w_vec;\n    \n    out[idx] = (T)result.x;\n    out[idx+1] = (T)result.y;\n    out[idx+2] = (T)result.z;\n    out[idx+3] = (T)result.w;\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_09fc2b3dc2ba",
                    "generation": 8,
                    "llm_reasoning": "Use float4 vectorization for maximum memory bandwidth (128-bit loads), processing 4*256=1024 elements per iteration across the threadgroup with careful alignment."
                  },
                  "visit_count": 1,
                  "max_reward": 1.1405943528570834,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      138.958,
                      124.333,
                      125.333,
                      151.625,
                      125.708,
                      124.042,
                      126.333,
                      125.916,
                      126.291,
                      128.125,
                      124.042,
                      128.833,
                      124.5,
                      122.292,
                      128.667,
                      127.75,
                      120.75,
                      120.709,
                      122.042,
                      125.25
                    ],
                    "median_us": 125.708,
                    "reward": 1.1405943528570834,
                    "speedup": 1.1514780284468769
                  },
                  "children": []
                },
                {
                  "node_id": "gen8_f1c8018566ed",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen8",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint PARTIALS = 16;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Accumulate into 16 register bins to maximize FMA throughput\nfloat partial[PARTIALS];\nfor (uint p = 0; p < PARTIALS; ++p) partial[p] = 0.0f;\n\nuint elements_per_thread = D / TG; // 2048/256 = 8\nuint start = tid * elements_per_thread;\n\nfor (uint i = 0; i < elements_per_thread; ++i) {\n    uint idx = base + start + i;\n    float v = (float)inp[idx];\n    partial[i % PARTIALS] += v * v;\n}\n\n// Tree reduce the partials in registers\nfloat sumsq = 0.0f;\nfor (uint p = 0; p < PARTIALS; ++p) sumsq += partial[p];\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Coalesced output with direct mapping\nfor (uint i = 0; i < elements_per_thread; ++i) {\n    uint idx = base + start + i;\n    uint w_idx = start + i;\n    out[idx] = (T)((float)inp[idx] * inv * (float)weight[w_idx]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_09fc2b3dc2ba",
                    "generation": 8,
                    "llm_reasoning": "Persistent threadgroup with register-based partial sums and minimal synchronization - compute 16 partial sums in registers before final reduction to reduce shared memory traffic."
                  },
                  "visit_count": 1,
                  "max_reward": 1.0028482694283658,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      125.791,
                      310.291,
                      176.041,
                      152.25,
                      150.334,
                      126.083,
                      123.709,
                      121.25,
                      127.959,
                      125.0,
                      127.958,
                      155.708,
                      134.583,
                      142.625,
                      146.0,
                      129.167,
                      125.916,
                      132.083,
                      212.5,
                      127.041
                    ],
                    "median_us": 132.083,
                    "reward": 1.0028482694283658,
                    "speedup": 1.095901819310585
                  },
                  "children": []
                },
                {
                  "node_id": "gen8_6686a9fea8c6",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen8",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[32]; // Only 32 slots for warp leaders\n\n// Phase 1: Each thread accumulates 8 values\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\n// SIMD-level reduction within each warp (32 threads)\nsumsq = simd_sum(sumsq);\n\n// Only lane 0 of each warp writes to shared memory\nuint warp_id = tid / 32;\nuint lane_id = tid % 32;\nif (lane_id == 0) {\n    buf[warp_id] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction by first warp only\nfloat final_sum = 0.0f;\nif (tid < 32) {\n    final_sum = (tid < 8) ? buf[tid] : 0.0f;\n    final_sum = simd_sum(final_sum);\n}\n\nfloat inv;\nif (tid == 0) {\n    buf[0] = metal::rsqrt(final_sum / (float)D + EPS);\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\ninv = buf[0];\n\n// Phase 2: Coalesced output\nfor (uint j = tid; j < D; j += TG) {\n    uint idx = base + j;\n    out[idx] = (T)((float)inp[idx] * inv * (float)weight[j]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_09fc2b3dc2ba",
                    "generation": 8,
                    "llm_reasoning": "Dual-pass SIMD reduction tree (32-wide warps \u2192 8 active threads \u2192 1 final sum) before threadgroup barrier, minimizing shared memory contention and maximizing SIMD utilization."
                  },
                  "visit_count": 1,
                  "max_reward": 1.0454055121013792,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      126.0,
                      129.083,
                      127.833,
                      141.042,
                      182.083,
                      276.209,
                      157.375,
                      137.084,
                      135.792,
                      128.959,
                      129.583,
                      128.959,
                      128.417,
                      127.583,
                      128.041,
                      121.958,
                      128.875,
                      132.791,
                      127.458,
                      129.125
                    ],
                    "median_us": 129.083,
                    "reward": 1.0454055121013792,
                    "speedup": 1.1213715206495045
                  },
                  "children": []
                },
                {
                  "node_id": "gen8_578bdbdd7950",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen8",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint TILE_SIZE = 256;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\nthreadgroup float weight_cache[TILE_SIZE];\n\n// Phase 1: Standard reduction\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Tile-based computation with cached weights\nuint num_tiles = D / TILE_SIZE; // 2048/256 = 8 tiles\n\nfor (uint tile = 0; tile < num_tiles; ++tile) {\n    uint tile_base = tile * TILE_SIZE;\n    \n    // Collaboratively load weight tile\n    weight_cache[tid] = (float)weight[tile_base + tid];\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    \n    // Compute outputs using cached weights\n    uint idx = base + tile_base + tid;\n    out[idx] = (T)((float)inp[idx] * inv * weight_cache[tid]);\n    \n    threadgroup_barrier(mem_flags::mem_threadgroup);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_09fc2b3dc2ba",
                    "generation": 8,
                    "llm_reasoning": "Tiled computation with threadgroup weight cache - load weights collaboratively into shared memory once, then reuse across all computations to eliminate redundant global reads."
                  },
                  "visit_count": 1,
                  "max_reward": 1.029914663230642,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      170.875,
                      148.25,
                      135.334,
                      131.458,
                      130.459,
                      136.5,
                      161.083,
                      142.708,
                      133.542,
                      132.625,
                      185.833,
                      140.167,
                      131.834,
                      137.375,
                      141.792,
                      137.5,
                      129.542,
                      133.708,
                      132.375,
                      131.25
                    ],
                    "median_us": 136.5,
                    "reward": 1.029914663230642,
                    "speedup": 1.0604395604395604
                  },
                  "children": []
                }
              ]
            },
            {
              "node_id": "gen3_8ff46ec0261b",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen3",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[128];  // Smaller buffer for better cache locality\n\n// Phase 1: Compute sum-of-squares\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\n// Two-stage reduction: first SIMD-level, then threadgroup\nsumsq = simd_sum(sumsq);\n\n// Only lane 0 of each SIMD group writes to shared memory\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0 && simd_group < 128) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction across SIMD groups\nif (tid < 8) {  // Only first SIMD group participates\n    float final_sum = (tid < 8) ? buf[tid] : 0.0f;\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Fused multiply - standard 4x unroll\nfor (uint j = tid; j < D; j += TG * 4) {\n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    out[base + j] = (T)((float)inp[base + j] * inv * w0);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * w1);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * w2);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * w3);\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_503ee400fc28",
                "generation": 3,
                "llm_reasoning": "Two-stage reduction with smaller threadgroup buffer (128 instead of 256) to reduce shared memory footprint and improve cache locality, combined with early termination for partial warps."
              },
              "visit_count": 5,
              "max_reward": 1.1372099453609432,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": true,
                "compile_error": null,
                "correctness_error": null,
                "timings_us": [
                  123.208,
                  126.0,
                  121.666,
                  122.791,
                  124.375,
                  154.75,
                  129.209,
                  140.375,
                  141.166,
                  131.333,
                  190.417,
                  157.875,
                  149.458,
                  174.166,
                  113.625,
                  147.0,
                  125.917,
                  135.75,
                  160.75,
                  150.5
                ],
                "median_us": 140.375,
                "reward": 1.0355201905995246,
                "speedup": 1.031166518254675
              },
              "children": [
                {
                  "node_id": "gen6_3a5e5cf5b201",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen6",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Compute sum-of-squares with aggressive prefetching\nfloat sumsq = 0.0f;\nfloat prefetch[4];\n\n// Prefetch first iteration\nfor (uint k = 0; k < 4; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        prefetch[k] = (float)inp[base + idx];\n    }\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    // Compute using prefetched values\n    sumsq += prefetch[0]*prefetch[0] + prefetch[1]*prefetch[1] + \n             prefetch[2]*prefetch[2] + prefetch[3]*prefetch[3];\n    \n    // Prefetch next iteration\n    uint next_j = j + TG * 4;\n    for (uint k = 0; k < 4; k++) {\n        uint idx = next_j + k * TG;\n        if (idx < D) {\n            prefetch[k] = (float)inp[base + idx];\n        }\n    }\n}\n\n// SIMD-level reduction only\nsumsq = simd_sum(sumsq);\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction\nif (tid < 8) {\n    float final_sum = buf[tid];\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Fused multiply with manual 4x unroll\nfor (uint j = tid; j < D; j += TG * 4) {\n    float4 w = float4((float)weight[j], (float)weight[j + TG], \n                      (float)weight[j + TG*2], (float)weight[j + TG*3]);\n    float4 v = float4((float)inp[base + j], (float)inp[base + j + TG],\n                      (float)inp[base + j + TG*2], (float)inp[base + j + TG*3]);\n    \n    v = v * inv * w;\n    \n    out[base + j] = (T)v.x;\n    out[base + j + TG] = (T)v.y;\n    out[base + j + TG*2] = (T)v.z;\n    out[base + j + TG*3] = (T)v.w;\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_8ff46ec0261b",
                    "generation": 6,
                    "llm_reasoning": "Speculative execution with warp-level reduction and deferred synchronization - compute output values speculatively before final reduction completes, then apply correction factor."
                  },
                  "visit_count": 1,
                  "max_reward": 0.9015279484547103,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      141.25,
                      172.25,
                      223.167,
                      472.584,
                      259.916,
                      154.75,
                      156.625,
                      135.583,
                      131.791,
                      126.75,
                      122.958,
                      147.833,
                      147.083,
                      137.875,
                      150.458,
                      194.0,
                      142.208,
                      138.167,
                      123.625,
                      135.5
                    ],
                    "median_us": 147.083,
                    "reward": 0.9015279484547103,
                    "speedup": 0.9841382076786577
                  },
                  "children": []
                },
                {
                  "node_id": "gen6_59e78975e6ce",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen6",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint CHUNK = 512;  // Process in 512-element chunks\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Process in chunks for better cache locality\nfloat sumsq = 0.0f;\n\nfor (uint chunk_start = 0; chunk_start < D; chunk_start += CHUNK) {\n    uint chunk_end = min(chunk_start + CHUNK, D);\n    \n    for (uint j = chunk_start + tid; j < chunk_end; j += TG) {\n        float v = (float)inp[base + j];\n        sumsq += v * v;\n    }\n}\n\n// SIMD reduction\nsumsq = simd_sum(sumsq);\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float final_sum = buf[tid];\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Write back with chunked access\nfor (uint chunk_start = 0; chunk_start < D; chunk_start += CHUNK) {\n    uint chunk_end = min(chunk_start + CHUNK, D);\n    \n    for (uint j = chunk_start + tid; j < chunk_end; j += TG) {\n        out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    }\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_8ff46ec0261b",
                    "generation": 6,
                    "llm_reasoning": "Minimize latency by computing in chunks with partial reductions - divide work into smaller chunks that fit in L1 cache, reducing memory latency."
                  },
                  "visit_count": 1,
                  "max_reward": 1.0664680071435142,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      135.584,
                      150.042,
                      145.333,
                      138.958,
                      134.083,
                      170.416,
                      144.25,
                      142.459,
                      128.875,
                      140.542,
                      132.875,
                      134.458,
                      128.375,
                      128.875,
                      128.5,
                      128.583,
                      127.125,
                      121.708,
                      129.917,
                      131.25
                    ],
                    "median_us": 134.083,
                    "reward": 1.0664680071435142,
                    "speedup": 1.0795552008830351
                  },
                  "children": []
                },
                {
                  "node_id": "gen6_6b12647487b2",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen6",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\n// Phase 1: Compute sum-of-squares with 8x unroll\nfloat sumsq = 0.0f;\n\nfor (uint j = tid; j < D; j += TG * 8) {\n    uint idx = base + j;\n    float v0 = (float)inp[idx];\n    float v1 = (float)inp[idx + TG];\n    float v2 = (float)inp[idx + TG*2];\n    float v3 = (float)inp[idx + TG*3];\n    float v4 = (float)inp[idx + TG*4];\n    float v5 = (float)inp[idx + TG*5];\n    float v6 = (float)inp[idx + TG*6];\n    float v7 = (float)inp[idx + TG*7];\n    \n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3 + v4*v4 + v5*v5 + v6*v6 + v7*v7;\n}\n\n// SIMD-level reduction\nsumsq = simd_sum(sumsq);\n\n// Compact cross-SIMD reduction using shuffle\nthreadgroup float shared_sum;\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    float group_sum = sumsq;\n    \n    // Manual reduction across 8 SIMD groups using atomic operations\n    if (simd_group == 0) {\n        shared_sum = 0.0f;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    \n    // Each SIMD group leader adds its contribution\n    threadgroup atomic_float* atomic_sum = (threadgroup atomic_float*)&shared_sum;\n    atomic_fetch_add_explicit(atomic_sum, group_sum, memory_order_relaxed);\n}\n\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(shared_sum / (float)D + EPS);\n\n// Phase 2: Fused multiply with 8x unroll\nfor (uint j = tid; j < D; j += TG * 8) {\n    uint idx = base + j;\n    \n    out[idx] = (T)((float)inp[idx] * inv * (float)weight[j]);\n    out[idx + TG] = (T)((float)inp[idx + TG] * inv * (float)weight[j + TG]);\n    out[idx + TG*2] = (T)((float)inp[idx + TG*2] * inv * (float)weight[j + TG*2]);\n    out[idx + TG*3] = (T)((float)inp[idx + TG*3] * inv * (float)weight[j + TG*3]);\n    out[idx + TG*4] = (T)((float)inp[idx + TG*4] * inv * (float)weight[j + TG*4]);\n    out[idx + TG*5] = (T)((float)inp[idx + TG*5] * inv * (float)weight[j + TG*5]);\n    out[idx + TG*6] = (T)((float)inp[idx + TG*6] * inv * (float)weight[j + TG*6]);\n    out[idx + TG*7] = (T)((float)inp[idx + TG*7] * inv * (float)weight[j + TG*7]);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_8ff46ec0261b",
                    "generation": 6,
                    "llm_reasoning": "Exploit asymmetric SIMD operations by using simd_shuffle for partial reduction tree, eliminating threadgroup memory for small reductions."
                  },
                  "visit_count": 1,
                  "max_reward": 0.0,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": false,
                    "compile_error": null,
                    "correctness_error": "[metal::Device] Unable to build metal library from source\nmlx/backend/metal/kernels/utils.h:537:5: error: no matching function for call to 'atomic_fetch_add_explicit'\n    atomic_fetch_add_explicit(atomic_sum, group_sum, memory_order_relaxed);\n    ^~~~~~~~~~~~~~~~~~~~~~~~~\n/System/Library/PrivateFrameworks/GPUCompiler.framework/Versions/32023/Libraries/lib/clang/32023.850/include/metal/metal_atomic:1351:14: note: candidate function [with T = float, U = float, _E = void] not viable: cannot pass pointer to address space 'threadgroup' as a pointer to address space 'device' in 1st argument\nMETAL_FUNC T atomic_fetch_add_explicit(\n             ^\n/System/Library/PrivateFrameworks/GPUCompiler.framework/Versions/32023/Libraries/lib/clang/32023.850/include/metal/metal_atomic:1343:14: note: candidate template ignored: requirement '_valid_fetch_add_type<threadgroup float *, void>::value' was not satisfied [with T = float, U = float]\nMETAL_FUNC T atomic_fetch_add_explicit(\n             ^\n\n",
                    "timings_us": [],
                    "median_us": Infinity,
                    "reward": 0.0,
                    "speedup": 0.0
                  },
                  "children": []
                },
                {
                  "node_id": "gen6_4ef1db3a0f7a",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen6",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Compute sum-of-squares with 4x unroll\nfloat sumsq = 0.0f;\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    \n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\n// SIMD reduction\nsumsq = simd_sum(sumsq);\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float final_sum = buf[tid];\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Prefetch and compute with register rotation\nfloat inp_reg[4];\nfloat weight_reg[4];\n\n// Prefetch first batch\nfor (uint k = 0; k < 4; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        inp_reg[k] = (float)inp[base + idx];\n        weight_reg[k] = (float)weight[idx];\n    }\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    // Compute using prefetched values\n    float result[4];\n    for (uint k = 0; k < 4; k++) {\n        result[k] = inp_reg[k] * inv * weight_reg[k];\n    }\n    \n    // Prefetch next batch while computing\n    uint next_j = j + TG * 4;\n    for (uint k = 0; k < 4; k++) {\n        uint idx = next_j + k * TG;\n        if (idx < D) {\n            inp_reg[k] = (float)inp[base + idx];\n            weight_reg[k] = (float)weight[idx];\n        }\n    }\n    \n    // Write results\n    out[base + j] = (T)result[0];\n    out[base + j + TG] = (T)result[1];\n    out[base + j + TG*2] = (T)result[2];\n    out[base + j + TG*3] = (T)result[3];\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_8ff46ec0261b",
                    "generation": 6,
                    "llm_reasoning": "Maximize memory bandwidth by using non-temporal stores (streamed writes) to avoid cache pollution, combined with prefetching input data into registers."
                  },
                  "visit_count": 1,
                  "max_reward": 1.1372099453609432,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      135.625,
                      128.084,
                      154.5,
                      136.75,
                      126.834,
                      125.875,
                      122.791,
                      121.875,
                      127.125,
                      119.833,
                      124.334,
                      106.666,
                      121.833,
                      131.709,
                      121.25,
                      123.541,
                      163.083,
                      110.833,
                      128.958,
                      125.542
                    ],
                    "median_us": 125.875,
                    "reward": 1.1372099453609432,
                    "speedup": 1.1499503475670307
                  },
                  "children": []
                }
              ]
            },
            {
              "node_id": "gen3_06c2ae930456",
              "candidate": {
                "spec": {
                  "name": "kk_glm_rmsnorm_gen3",
                  "input_names": [
                    "inp",
                    "weight"
                  ],
                  "output_names": [
                    "out"
                  ],
                  "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Standard reduction\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Memory-level parallelism by batching loads\nfor (uint j = tid; j < D; j += TG * 4) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    \n    // Batch all loads first to maximize MLP\n    float in0 = (float)inp[idx0];\n    float in1 = (float)inp[idx1];\n    float in2 = (float)inp[idx2];\n    float in3 = (float)inp[idx3];\n    \n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    // Compute all FMAs\n    float res0 = in0 * inv * w0;\n    float res1 = in1 * inv * w1;\n    float res2 = in2 * inv * w2;\n    float res3 = in3 * inv * w3;\n    \n    // Batch all stores\n    out[idx0] = (T)res0;\n    out[idx1] = (T)res1;\n    out[idx2] = (T)res2;\n    out[idx3] = (T)res3;\n}",
                  "header": "",
                  "threadgroup": [
                    256,
                    1,
                    1
                  ],
                  "template_params": [
                    [
                      "T",
                      "float32"
                    ]
                  ]
                },
                "parent_id": "gen1_503ee400fc28",
                "generation": 3,
                "llm_reasoning": "Exploit memory-level parallelism by interleaving loads for both input and weight in the second phase, computing results in a separate pass to maximize cache hits and hide latency."
              },
              "visit_count": 5,
              "max_reward": 1.0283112556672642,
              "prior": 0.25,
              "eval_result": {
                "compiled": true,
                "correct": true,
                "compile_error": null,
                "correctness_error": null,
                "timings_us": [
                  161.542,
                  150.292,
                  140.0,
                  131.375,
                  132.125,
                  148.084,
                  152.875,
                  148.417,
                  147.334,
                  150.917,
                  148.958,
                  148.959,
                  136.25,
                  145.875,
                  148.125,
                  150.833,
                  201.291,
                  135.75,
                  129.708,
                  123.833
                ],
                "median_us": 148.125,
                "reward": 0.9922263266104029,
                "speedup": 0.9772151898734177
              },
              "children": [
                {
                  "node_id": "gen7_bd69b0335d15",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen7",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Reduction with float2 vectorization\nfloat sumsq = 0.0f;\nfor (uint j = tid * 2; j < D; j += TG * 8) {\n    float2 v0 = float2((float)inp[base + j], (float)inp[base + j + 1]);\n    float2 v1 = float2((float)inp[base + j + TG*2], (float)inp[base + j + TG*2 + 1]);\n    float2 v2 = float2((float)inp[base + j + TG*4], (float)inp[base + j + TG*4 + 1]);\n    float2 v3 = float2((float)inp[base + j + TG*6], (float)inp[base + j + TG*6 + 1]);\n    sumsq += v0.x*v0.x + v0.y*v0.y + v1.x*v1.x + v1.y*v1.y + v2.x*v2.x + v2.y*v2.y + v3.x*v3.x + v3.y*v3.y;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: float2 vectorized output\nfor (uint j = tid * 2; j < D; j += TG * 8) {\n    float2 in0 = float2((float)inp[base + j], (float)inp[base + j + 1]);\n    float2 in1 = float2((float)inp[base + j + TG*2], (float)inp[base + j + TG*2 + 1]);\n    float2 in2 = float2((float)inp[base + j + TG*4], (float)inp[base + j + TG*4 + 1]);\n    float2 in3 = float2((float)inp[base + j + TG*6], (float)inp[base + j + TG*6 + 1]);\n    \n    float2 w0 = float2((float)weight[j], (float)weight[j + 1]);\n    float2 w1 = float2((float)weight[j + TG*2], (float)weight[j + TG*2 + 1]);\n    float2 w2 = float2((float)weight[j + TG*4], (float)weight[j + TG*4 + 1]);\n    float2 w3 = float2((float)weight[j + TG*6], (float)weight[j + TG*6 + 1]);\n    \n    out[base + j] = (T)(in0.x * inv * w0.x);\n    out[base + j + 1] = (T)(in0.y * inv * w0.y);\n    out[base + j + TG*2] = (T)(in1.x * inv * w1.x);\n    out[base + j + TG*2 + 1] = (T)(in1.y * inv * w1.y);\n    out[base + j + TG*4] = (T)(in2.x * inv * w2.x);\n    out[base + j + TG*4 + 1] = (T)(in2.y * inv * w2.y);\n    out[base + j + TG*6] = (T)(in3.x * inv * w3.x);\n    out[base + j + TG*6 + 1] = (T)(in3.y * inv * w3.y);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_06c2ae930456",
                    "generation": 7,
                    "llm_reasoning": "Use float2 vectorization (safer than float4) with interleaved computation to maximize memory bandwidth while maintaining alignment safety."
                  },
                  "visit_count": 1,
                  "max_reward": 1.0188984595152384,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      135.667,
                      133.375,
                      130.792,
                      132.291,
                      130.625,
                      131.042,
                      131.708,
                      129.959,
                      130.042,
                      130.875,
                      130.25,
                      135.083,
                      132.959,
                      132.458,
                      129.333,
                      129.916,
                      128.375,
                      161.959,
                      138.333,
                      479.667
                    ],
                    "median_us": 131.708,
                    "reward": 1.0188984595152384,
                    "speedup": 1.099022079144775
                  },
                  "children": []
                },
                {
                  "node_id": "gen7_42df2faca8f4",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen7",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\nthreadgroup float cached_weights[D];\n\n// Phase 1: Reduction while caching weights\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n    cached_weights[j] = (float)weight[j];\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Use cached weights (no global weight loads)\nfor (uint j = tid; j < D; j += TG * 4) {\n    float in0 = (float)inp[base + j];\n    float in1 = (float)inp[base + j + TG];\n    float in2 = (float)inp[base + j + TG*2];\n    float in3 = (float)inp[base + j + TG*3];\n    \n    float w0 = cached_weights[j];\n    float w1 = cached_weights[j + TG];\n    float w2 = cached_weights[j + TG*2];\n    float w3 = cached_weights[j + TG*3];\n    \n    out[base + j] = (T)(in0 * inv * w0);\n    out[base + j + TG] = (T)(in1 * inv * w1);\n    out[base + j + TG*2] = (T)(in2 * inv * w2);\n    out[base + j + TG*3] = (T)(in3 * inv * w3);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_06c2ae930456",
                    "generation": 7,
                    "llm_reasoning": "Cache weight values in threadgroup memory during reduction phase to eliminate repeated global loads in the second phase."
                  },
                  "visit_count": 1,
                  "max_reward": 1.0283112556672642,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": true,
                    "compile_error": null,
                    "correctness_error": null,
                    "timings_us": [
                      161.959,
                      122.792,
                      124.834,
                      123.041,
                      122.208,
                      184.25,
                      128.291,
                      162.708,
                      160.625,
                      133.542,
                      134.083,
                      132.375,
                      130.791,
                      128.417,
                      138.083,
                      198.208,
                      155.625,
                      132.125,
                      128.375,
                      140.667
                    ],
                    "median_us": 133.542,
                    "reward": 1.0283112556672642,
                    "speedup": 1.0839286516601518
                  },
                  "children": []
                },
                {
                  "node_id": "gen7_a92920dd510a",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen7",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: 6-way unrolled reduction\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 6) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    float v4 = (float)inp[base + j + TG*4];\n    float v5 = (float)inp[base + j + TG*5];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3 + v4*v4 + v5*v5;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: 6-way unrolled computation\nfor (uint j = tid; j < D; j += TG * 6) {\n    float in0 = (float)inp[base + j];\n    float in1 = (float)inp[base + j + TG];\n    float in2 = (float)inp[base + j + TG*2];\n    float in3 = (float)inp[base + j + TG*3];\n    float in4 = (float)inp[base + j + TG*4];\n    float in5 = (float)inp[base + j + TG*5];\n    \n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    float w4 = (float)weight[j + TG*4];\n    float w5 = (float)weight[j + TG*5];\n    \n    out[base + j] = (T)(in0 * inv * w0);\n    out[base + j + TG] = (T)(in1 * inv * w1);\n    out[base + j + TG*2] = (T)(in2 * inv * w2);\n    out[base + j + TG*3] = (T)(in3 * inv * w3);\n    out[base + j + TG*4] = (T)(in4 * inv * w4);\n    out[base + j + TG*5] = (T)(in5 * inv * w5);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_06c2ae930456",
                    "generation": 7,
                    "llm_reasoning": "Two-level parallelism: 6-way unroll in reduction and 6-way unroll in computation to better match the 10 GPU cores on M4."
                  },
                  "visit_count": 1,
                  "max_reward": 0.0,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": false,
                    "compile_error": null,
                    "correctness_error": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\nOutput 0 mismatch\nMismatched elements: 2046 / 2048 (99.9%)\nFirst 5 mismatches are at indices:\n [0, 0]: -2.3107564449310303 (ACTUAL), -2.82169508934021 (DESIRED)\n [0, 1]: -0.00818962324410677 (ACTUAL), -0.010000457055866718 (DESIRED)\n [0, 2]: -0.42910104990005493 (ACTUAL), -0.523980975151062 (DESIRED)\n [0, 3]: -0.018458209931850433 (ACTUAL), -0.022539563477039337 (DESIRED)\n [0, 4]: -1.1944130659103394 (ACTUAL), -1.4585134983062744 (DESIRED)\nMax absolute difference among violations: 1.4761014\nMax relative difference among violations: 0.1810752\n ACTUAL: array([[-2.310756, -0.00819 , -0.429101, ..., -2.272972, -0.35736 ,\n         0.598132]], shape=(1, 2048), dtype=float32)\n DESIRED: array([[-2.821695, -0.01    , -0.523981, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)",
                    "timings_us": [],
                    "median_us": Infinity,
                    "reward": 0.0,
                    "speedup": 0.0
                  },
                  "children": []
                },
                {
                  "node_id": "gen7_d584e9c4a843",
                  "candidate": {
                    "spec": {
                      "name": "kk_glm_rmsnorm_gen7",
                      "input_names": [
                        "inp",
                        "weight"
                      ],
                      "output_names": [
                        "out"
                      ],
                      "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Reduction with SIMD-only path\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\n// SIMD-level reduction (within each warp of 32)\nfloat simd_sum = simd_sum(sumsq);\n\n// Only one thread per SIMD group writes to shared memory\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sum;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction of 8 values (256/32 = 8 SIMD groups)\nfloat total = 0.0f;\nif (tid < 8) {\n    total = buf[tid];\n}\ntotal = simd_sum(total);\nfloat inv = metal::rsqrt(total / (float)D + EPS);\n\n// Phase 2: Standard computation\nfor (uint j = tid; j < D; j += TG * 4) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    \n    float in0 = (float)inp[idx0];\n    float in1 = (float)inp[idx1];\n    float in2 = (float)inp[idx2];\n    float in3 = (float)inp[idx3];\n    \n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    out[idx0] = (T)(in0 * inv * w0);\n    out[idx1] = (T)(in1 * inv * w1);\n    out[idx2] = (T)(in2 * inv * w2);\n    out[idx3] = (T)(in3 * inv * w3);\n}",
                      "header": "",
                      "threadgroup": [
                        256,
                        1,
                        1
                      ],
                      "template_params": [
                        [
                          "T",
                          "float32"
                        ]
                      ]
                    },
                    "parent_id": "gen3_06c2ae930456",
                    "generation": 7,
                    "llm_reasoning": "Reduce synchronization cost by using SIMD shuffle-based reduction instead of threadgroup memory, eliminating one barrier."
                  },
                  "visit_count": 1,
                  "max_reward": 0.0,
                  "prior": 0.25,
                  "eval_result": {
                    "compiled": true,
                    "correct": false,
                    "compile_error": null,
                    "correctness_error": "[metal::Device] Unable to build metal library from source\nmlx/backend/metal/kernels/utils.h:514:26: error: called object type 'float' is not a function or function pointer\nfloat simd_sum = simd_sum(sumsq);\n                 ~~~~~~~~^\nmlx/backend/metal/kernels/utils.h:529:17: error: called object type 'float' is not a function or function pointer\ntotal = simd_sum(total);\n        ~~~~~~~~^\n\n",
                    "timings_us": [],
                    "median_us": Infinity,
                    "reward": 0.0,
                    "speedup": 0.0
                  },
                  "children": []
                }
              ]
            }
          ]
        },
        {
          "node_id": "gen1_a0985b7124b7",
          "candidate": {
            "spec": {
              "name": "kk_glm_rmsnorm_gen1",
              "input_names": [
                "inp",
                "weight"
              ],
              "output_names": [
                "out"
              ],
              "source": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Vectorized sum-of-squares with 2x unroll\nfloat sumsq = 0.0f;\nfor (uint j = tid * 2; j < D; j += TG * 4) {\n    float2 v0 = *((device float2*)&inp[base + j]);\n    float2 v1 = *((device float2*)&inp[base + j + TG*2]);\n    sumsq += v0.x * v0.x + v0.y * v0.y + v1.x * v1.x + v1.y * v1.y;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Vectorized output with 2x unroll\nfor (uint j = tid * 2; j < D; j += TG * 4) {\n    float2 v0 = *((device float2*)&inp[base + j]);\n    float2 w0 = *((device float2*)&weight[j]);\n    float2 result0 = v0 * inv * w0;\n    *((device float2*)&out[base + j]) = (float2)((T)result0.x, (T)result0.y);\n    \n    float2 v1 = *((device float2*)&inp[base + j + TG*2]);\n    float2 w1 = *((device float2*)&weight[j + TG*2]);\n    float2 result1 = v1 * inv * w1;\n    *((device float2*)&out[base + j + TG*2]) = (float2)((T)result1.x, (T)result1.y);\n}",
              "header": "",
              "threadgroup": [
                256,
                1,
                1
              ],
              "template_params": [
                [
                  "T",
                  "float32"
                ]
              ]
            },
            "parent_id": "root",
            "generation": 1,
            "llm_reasoning": "Combined vectorization (float2) with 2x loop unroll, balancing memory bandwidth and instruction-level parallelism."
          },
          "visit_count": 1,
          "max_reward": 0.0,
          "prior": 0.25,
          "eval_result": {
            "compiled": true,
            "correct": false,
            "compile_error": null,
            "correctness_error": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\nOutput 0 mismatch\nMismatched elements: 1024 / 2048 (50%)\nFirst 5 mismatches are at indices:\n [0, 0]: -0.010000457055866718 (ACTUAL), -2.82169508934021 (DESIRED)\n [0, 2]: -0.022539563477039337 (ACTUAL), -0.523980975151062 (DESIRED)\n [0, 4]: -0.21335050463676453 (ACTUAL), -1.4585134983062744 (DESIRED)\n [0, 6]: 0.09140914678573608 (ACTUAL), -0.010970774106681347 (DESIRED)\n [0, 8]: 1.0492185354232788 (ACTUAL), 0.21376259624958038 (DESIRED)\nMax absolute difference among violations: 8.133413\nMax relative difference among violations: 1948.6418\n ACTUAL: array([[-0.01    , -0.01    , -0.02254 , ..., -2.775556,  0.730387,\n         0.730387]], shape=(1, 2048), dtype=float32)\n DESIRED: array([[-2.821695, -0.01    , -0.523981, ..., -2.775556, -0.436377,\n         0.730387]], shape=(1, 2048), dtype=float32)",
            "timings_us": [],
            "median_us": Infinity,
            "reward": 0.0,
            "speedup": 0.0
          },
          "children": []
        }
      ]
    }
  },
  "eval_history": [],
  "candidate_sources": {
    "gen1_ed96515fe09c": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Vectorized sum-of-squares computation\nfloat sumsq = 0.0f;\nfor (uint j = tid * 4; j < D; j += TG * 4) {\n    float4 v = *((device float4*)&inp[base + j]);\n    sumsq += v.x * v.x + v.y * v.y + v.z * v.z + v.w * v.w;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Vectorized output computation\nfor (uint j = tid * 4; j < D; j += TG * 4) {\n    float4 v = *((device float4*)&inp[base + j]);\n    float4 w = *((device float4*)&weight[j]);\n    float4 result = v * inv * w;\n    *((device float4*)&out[base + j]) = (float4)((T)result.x, (T)result.y, (T)result.z, (T)result.w);\n}",
    "gen1_8d1551be6855": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint UNROLL = 8;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * UNROLL) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    float v4 = (float)inp[base + j + TG*4];\n    float v5 = (float)inp[base + j + TG*5];\n    float v6 = (float)inp[base + j + TG*6];\n    float v7 = (float)inp[base + j + TG*7];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3 + v4*v4 + v5*v5 + v6*v6 + v7*v7;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j = tid; j < D; j += TG * UNROLL) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    float v4 = (float)inp[base + j + TG*4];\n    float v5 = (float)inp[base + j + TG*5];\n    float v6 = (float)inp[base + j + TG*6];\n    float v7 = (float)inp[base + j + TG*7];\n    \n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * (float)weight[j + TG]);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * (float)weight[j + TG*2]);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * (float)weight[j + TG*3]);\n    out[base + j + TG*4] = (T)((float)inp[base + j + TG*4] * inv * (float)weight[j + TG*4]);\n    out[base + j + TG*5] = (T)((float)inp[base + j + TG*5] * inv * (float)weight[j + TG*5]);\n    out[base + j + TG*6] = (T)((float)inp[base + j + TG*6] * inv * (float)weight[j + TG*6]);\n    out[base + j + TG*7] = (T)((float)inp[base + j + TG*7] * inv * (float)weight[j + TG*7]);\n}",
    "gen1_503ee400fc28": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Compute sum-of-squares with 4x unroll\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Fused multiply with register reuse\nfor (uint j = tid; j < D; j += TG * 4) {\n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    out[base + j] = (T)((float)inp[base + j] * inv * w0);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * w1);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * w2);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * w3);\n}",
    "gen1_a0985b7124b7": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Vectorized sum-of-squares with 2x unroll\nfloat sumsq = 0.0f;\nfor (uint j = tid * 2; j < D; j += TG * 4) {\n    float2 v0 = *((device float2*)&inp[base + j]);\n    float2 v1 = *((device float2*)&inp[base + j + TG*2]);\n    sumsq += v0.x * v0.x + v0.y * v0.y + v1.x * v1.x + v1.y * v1.y;\n}\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Vectorized output with 2x unroll\nfor (uint j = tid * 2; j < D; j += TG * 4) {\n    float2 v0 = *((device float2*)&inp[base + j]);\n    float2 w0 = *((device float2*)&weight[j]);\n    float2 result0 = v0 * inv * w0;\n    *((device float2*)&out[base + j]) = (float2)((T)result0.x, (T)result0.y);\n    \n    float2 v1 = *((device float2*)&inp[base + j + TG*2]);\n    float2 w1 = *((device float2*)&weight[j + TG*2]);\n    float2 result1 = v1 * inv * w1;\n    *((device float2*)&out[base + j + TG*2]) = (float2)((T)result1.x, (T)result1.y);\n}",
    "gen2_adb8f750d639": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\n// SIMD-level reduction first\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction across SIMD groups\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
    "gen2_ccd8740b6477": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat w_cache[8];\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        w_cache[k] = (float)weight[idx];\n    }\n}\n\nfloat sumsq = 0.0f;\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        sumsq += v * v;\n    }\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)((float)inp[base + idx] * inv * w_cache[k]);\n    }\n}",
    "gen2_b28fbd535738": "constexpr uint D = 2048;\nconstexpr uint TG = 512;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v0 = (float)inp[base + j];\n    float v1 = (j + TG < D) ? (float)inp[base + j + TG] : 0.0f;\n    float v2 = (j + TG*2 < D) ? (float)inp[base + j + TG*2] : 0.0f;\n    float v3 = (j + TG*3 < D) ? (float)inp[base + j + TG*3] : 0.0f;\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j = tid; j < D; j += TG) {\n    if (j < D) out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    if (j + TG < D) out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * (float)weight[j + TG]);\n    if (j + TG*2 < D) out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * (float)weight[j + TG*2]);\n    if (j + TG*3 < D) out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * (float)weight[j + TG*3]);\n}",
    "gen2_f6cd826132fb": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float sumsq_buf[TG];\nthreadgroup float val_buf[D];\n\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    val_buf[j] = v;\n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(sumsq_buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(sumsq_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)(val_buf[j] * inv * (float)weight[j]);\n}",
    "gen3_1b04b6c690c7": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Compute sum-of-squares with manual software pipelining\nfloat sumsq = 0.0f;\nfloat prefetch_v0, prefetch_v1, prefetch_v2, prefetch_v3;\n\n// Prefetch first iteration\nif (tid < D) {\n    prefetch_v0 = (float)inp[base + tid];\n    prefetch_v1 = (float)inp[base + tid + TG];\n    prefetch_v2 = (float)inp[base + tid + TG*2];\n    prefetch_v3 = (float)inp[base + tid + TG*3];\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    // Compute with prefetched values\n    float v0 = prefetch_v0;\n    float v1 = prefetch_v1;\n    float v2 = prefetch_v2;\n    float v3 = prefetch_v3;\n    \n    // Prefetch next iteration while computing current\n    uint next_j = j + TG * 4;\n    if (next_j < D) {\n        prefetch_v0 = (float)inp[base + next_j];\n        prefetch_v1 = (float)inp[base + next_j + TG];\n        prefetch_v2 = (float)inp[base + next_j + TG*2];\n        prefetch_v3 = (float)inp[base + next_j + TG*3];\n    }\n    \n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Fused multiply with same pipelining strategy\nif (tid < D) {\n    prefetch_v0 = (float)inp[base + tid];\n    prefetch_v1 = (float)weight[tid];\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    float in0 = prefetch_v0;\n    float w0 = prefetch_v1;\n    \n    uint idx1 = j + TG;\n    uint idx2 = j + TG*2;\n    uint idx3 = j + TG*3;\n    \n    float in1 = (float)inp[base + idx1];\n    float w1 = (float)weight[idx1];\n    float in2 = (float)inp[base + idx2];\n    float w2 = (float)weight[idx2];\n    float in3 = (float)inp[base + idx3];\n    float w3 = (float)weight[idx3];\n    \n    // Prefetch next iteration\n    uint next_j = j + TG * 4;\n    if (next_j < D) {\n        prefetch_v0 = (float)inp[base + next_j];\n        prefetch_v1 = (float)weight[next_j];\n    }\n    \n    out[base + j] = (T)(in0 * inv * w0);\n    out[base + idx1] = (T)(in1 * inv * w1);\n    out[base + idx2] = (T)(in2 * inv * w2);\n    out[base + idx3] = (T)(in3 * inv * w3);\n}",
    "gen3_09fc2b3dc2ba": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Each thread processes 8 elements per iteration in registers\nfloat sumsq = 0.0f;\nuint stride = TG * 8;\n\nfor (uint j = tid; j < D; j += stride) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    uint idx4 = idx0 + TG*4;\n    uint idx5 = idx0 + TG*5;\n    uint idx6 = idx0 + TG*6;\n    uint idx7 = idx0 + TG*7;\n    \n    float v0 = (float)inp[idx0];\n    float v1 = (float)inp[idx1];\n    float v2 = (float)inp[idx2];\n    float v3 = (float)inp[idx3];\n    float v4 = (float)inp[idx4];\n    float v5 = (float)inp[idx5];\n    float v6 = (float)inp[idx6];\n    float v7 = (float)inp[idx7];\n    \n    // Accumulate in pairs to maximize FMA usage\n    float sum01 = v0*v0 + v1*v1;\n    float sum23 = v2*v2 + v3*v3;\n    float sum45 = v4*v4 + v5*v5;\n    float sum67 = v6*v6 + v7*v7;\n    \n    sumsq += (sum01 + sum23) + (sum45 + sum67);\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Same 8-element blocking for output\nfor (uint j = tid; j < D; j += stride) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    uint idx4 = idx0 + TG*4;\n    uint idx5 = idx0 + TG*5;\n    uint idx6 = idx0 + TG*6;\n    uint idx7 = idx0 + TG*7;\n    \n    out[idx0] = (T)((float)inp[idx0] * inv * (float)weight[j]);\n    out[idx1] = (T)((float)inp[idx1] * inv * (float)weight[j + TG]);\n    out[idx2] = (T)((float)inp[idx2] * inv * (float)weight[j + TG*2]);\n    out[idx3] = (T)((float)inp[idx3] * inv * (float)weight[j + TG*3]);\n    out[idx4] = (T)((float)inp[idx4] * inv * (float)weight[j + TG*4]);\n    out[idx5] = (T)((float)inp[idx5] * inv * (float)weight[j + TG*5]);\n    out[idx6] = (T)((float)inp[idx6] * inv * (float)weight[j + TG*6]);\n    out[idx7] = (T)((float)inp[idx7] * inv * (float)weight[j + TG*7]);\n}",
    "gen3_8ff46ec0261b": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[128];  // Smaller buffer for better cache locality\n\n// Phase 1: Compute sum-of-squares\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\n// Two-stage reduction: first SIMD-level, then threadgroup\nsumsq = simd_sum(sumsq);\n\n// Only lane 0 of each SIMD group writes to shared memory\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0 && simd_group < 128) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction across SIMD groups\nif (tid < 8) {  // Only first SIMD group participates\n    float final_sum = (tid < 8) ? buf[tid] : 0.0f;\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Fused multiply - standard 4x unroll\nfor (uint j = tid; j < D; j += TG * 4) {\n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    out[base + j] = (T)((float)inp[base + j] * inv * w0);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * w1);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * w2);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * w3);\n}",
    "gen3_06c2ae930456": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Standard reduction\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Memory-level parallelism by batching loads\nfor (uint j = tid; j < D; j += TG * 4) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    \n    // Batch all loads first to maximize MLP\n    float in0 = (float)inp[idx0];\n    float in1 = (float)inp[idx1];\n    float in2 = (float)inp[idx2];\n    float in3 = (float)inp[idx3];\n    \n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    // Compute all FMAs\n    float res0 = in0 * inv * w0;\n    float res1 = in1 * inv * w1;\n    float res2 = in2 * inv * w2;\n    float res3 = in3 * inv * w3;\n    \n    // Batch all stores\n    out[idx0] = (T)res0;\n    out[idx1] = (T)res1;\n    out[idx2] = (T)res2;\n    out[idx3] = (T)res3;\n}",
    "gen4_aa150a4a2bc7": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG + 8]; // padding to reduce bank conflicts\n\nfloat w_cache[8];\nfloat x_cache[8];\nfloat sumsq = 0.0f;\n\n#pragma unroll\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        float w = (float)weight[idx];\n        w_cache[k] = w;\n        x_cache[k] = v;\n        sumsq += v * v;\n    }\n}\n\nbuf[tid] = sumsq;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 128) buf[tid] += buf[tid + 128];\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nif (tid < 64) buf[tid] += buf[tid + 64];\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nif (tid < 32) buf[tid] += buf[tid + 32];\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 32) {\n    float val = buf[tid] + buf[tid + 16];\n    val += metal::simd_shuffle_down(val, 8);\n    val += metal::simd_shuffle_down(val, 4);\n    val += metal::simd_shuffle_down(val, 2);\n    val += metal::simd_shuffle_down(val, 1);\n    if (tid == 0) buf[0] = val;\n}\n\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n#pragma unroll\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
    "gen4_bda0aae130b0": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[32]; // only need per-warp storage\n\nfloat w_cache[8];\nfloat x_cache[8];\nfloat sumsq = 0.0f;\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        float w = (float)weight[idx];\n        w_cache[k] = w;\n        x_cache[k] = v;\n        sumsq += v * v;\n    }\n}\n\n// SIMD-level reduction first\nfloat simd_sum_val = metal::simd_sum(sumsq);\n\nuint warp_id = tid / 32;\nuint lane_id = tid % 32;\n\nif (lane_id == 0) {\n    buf[warp_id] = simd_sum_val;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction across warps\nfloat total = 0.0f;\nif (tid < 32) {\n    total = (tid < 8) ? buf[tid] : 0.0f;\n    total = metal::simd_sum(total);\n    if (tid == 0) buf[0] = total;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
    "gen4_7f0e47ee7586": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\nfloat w_cache[8];\nfloat x_cache[8];\n\n// Prefetch first batch\nuint idx0 = tid;\nif (idx0 < D) {\n    x_cache[0] = (float)inp[base + idx0];\n    w_cache[0] = (float)weight[idx0];\n}\n\nfloat sumsq = 0.0f;\nfor (uint k = 0; k < 7; k++) {\n    uint idx_next = tid + (k + 1) * TG;\n    \n    // Prefetch next while computing current\n    if (idx_next < D) {\n        x_cache[k + 1] = (float)inp[base + idx_next];\n        w_cache[k + 1] = (float)weight[idx_next];\n    }\n    \n    // Compute current\n    uint idx_curr = tid + k * TG;\n    if (idx_curr < D) {\n        float v = x_cache[k];\n        sumsq += v * v;\n    }\n}\n\n// Last iteration\nuint idx_last = tid + 7 * TG;\nif (idx_last < D) {\n    float v = x_cache[7];\n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
    "gen4_3687624aa661": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float atomic_sum;\nthreadgroup float buf[TG];\n\nif (tid == 0) atomic_sum = 0.0f;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat w_cache[8];\nfloat x_cache[8];\nfloat sumsq = 0.0f;\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        float v = (float)inp[base + idx];\n        float w = (float)weight[idx];\n        w_cache[k] = w;\n        x_cache[k] = v;\n        sumsq += v * v;\n    }\n}\n\n// Warp-level reduction only\nfloat warp_sum = metal::simd_sum(sumsq);\nuint lane = tid % 32;\n\nif (lane == 0) {\n    buf[tid / 32] = warp_sum;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Single warp does final reduction\nfloat total = 0.0f;\nif (tid < 8) {\n    total = buf[tid] + metal::simd_shuffle_down(buf[tid], 4);\n    total += metal::simd_shuffle_down(total, 2);\n    total += metal::simd_shuffle_down(total, 1);\n    if (tid == 0) atomic_sum = total;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(atomic_sum / (float)D + EPS);\n\nfor (uint k = 0; k < 8; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        out[base + idx] = (T)(x_cache[k] * inv * w_cache[k]);\n    }\n}",
    "gen5_b893f97c23b4": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Sum-of-squares with 4-way unroll\nfloat sumsq = 0.0f;\nuint j = tid;\nfor (; j + TG*3 < D; j += TG*4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n// Remainder\nfor (; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\n// SIMD reduction\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Apply normalization with 4-way unroll\nj = tid;\nfor (; j + TG*3 < D; j += TG*4) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    out[base + j + TG] = (T)((float)inp[base + j + TG] * inv * (float)weight[j + TG]);\n    out[base + j + TG*2] = (T)((float)inp[base + j + TG*2] * inv * (float)weight[j + TG*2]);\n    out[base + j + TG*3] = (T)((float)inp[base + j + TG*3] * inv * (float)weight[j + TG*3]);\n}\nfor (; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
    "gen5_d3b2676b63d8": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint CHUNK = 16;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Accumulate sum-of-squares with large chunks\nfloat sumsq = 0.0f;\nfor (uint block = 0; block < D / CHUNK; block++) {\n    uint offset = block * CHUNK;\n    if (offset + tid < D) {\n        float acc0 = 0.0f, acc1 = 0.0f;\n        for (uint i = 0; i < CHUNK && offset + tid + i*TG < D; i += 2) {\n            float v0 = (float)inp[base + offset + tid + i*TG];\n            float v1 = (i+1 < CHUNK && offset + tid + (i+1)*TG < D) ? (float)inp[base + offset + tid + (i+1)*TG] : 0.0f;\n            acc0 += v0 * v0;\n            acc1 += v1 * v1;\n        }\n        sumsq += acc0 + acc1;\n    }\n}\n\n// Single-barrier SIMD reduction\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat total = 0.0f;\nif (tid < 8) {\n    total = simd_sum(buf[tid]);\n}\nfloat inv = metal::rsqrt(total / (float)D + EPS);\n\n// Write output\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
    "gen5_5e5134603ba3": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\nfloat cached[8];  // Cache input values in registers\n\n// Load and accumulate in same pass\nfloat sumsq = 0.0f;\nuint cache_idx = 0;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    if (cache_idx < 8) {\n        cached[cache_idx++] = v;\n    }\n    sumsq += v * v;\n}\n\n// Two-stage reduction\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Write cached values first (no load needed)\ncache_idx = 0;\nfor (uint j = tid; j < D && cache_idx < 8; j += TG, cache_idx++) {\n    out[base + j] = (T)(cached[cache_idx] * inv * (float)weight[j]);\n}\n\n// Handle remaining elements\nfor (uint j = tid + 8*TG; j < D; j += TG) {\n    out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n}",
    "gen5_bcacf285d98a": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint simd_lane = tid & 31;\nuint simd_group = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Process with SIMD-aligned strides (32 elements at a time)\nfloat sumsq = 0.0f;\nfor (uint block = simd_group; block < D/32; block += 8) {\n    uint idx = block * 32 + simd_lane;\n    float v = (float)inp[base + idx];\n    sumsq += v * v;\n}\n\n// Handle remainder if D not multiple of 256\nif (simd_group == 7) {\n    for (uint idx = 224 + simd_lane; idx < D; idx += 32) {\n        float v = (float)inp[base + idx];\n        sumsq += v * v;\n    }\n}\n\n// SIMD reduction within each warp\nfloat simd_sumsq = simd_sum(sumsq);\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final cross-warp reduction\nif (tid < 8) {\n    float val = buf[tid];\n    val = simd_sum(val);\n    if (tid == 0) {\n        buf[0] = val;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Write output with same SIMD-aligned pattern\nfor (uint block = simd_group; block < D/32; block += 8) {\n    uint idx = block * 32 + simd_lane;\n    out[base + idx] = (T)((float)inp[base + idx] * inv * (float)weight[idx]);\n}\nif (simd_group == 7) {\n    for (uint idx = 224 + simd_lane; idx < D; idx += 32) {\n        out[base + idx] = (T)((float)inp[base + idx] * inv * (float)weight[idx]);\n    }\n}",
    "gen6_3a5e5cf5b201": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Compute sum-of-squares with aggressive prefetching\nfloat sumsq = 0.0f;\nfloat prefetch[4];\n\n// Prefetch first iteration\nfor (uint k = 0; k < 4; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        prefetch[k] = (float)inp[base + idx];\n    }\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    // Compute using prefetched values\n    sumsq += prefetch[0]*prefetch[0] + prefetch[1]*prefetch[1] + \n             prefetch[2]*prefetch[2] + prefetch[3]*prefetch[3];\n    \n    // Prefetch next iteration\n    uint next_j = j + TG * 4;\n    for (uint k = 0; k < 4; k++) {\n        uint idx = next_j + k * TG;\n        if (idx < D) {\n            prefetch[k] = (float)inp[base + idx];\n        }\n    }\n}\n\n// SIMD-level reduction only\nsumsq = simd_sum(sumsq);\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction\nif (tid < 8) {\n    float final_sum = buf[tid];\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Fused multiply with manual 4x unroll\nfor (uint j = tid; j < D; j += TG * 4) {\n    float4 w = float4((float)weight[j], (float)weight[j + TG], \n                      (float)weight[j + TG*2], (float)weight[j + TG*3]);\n    float4 v = float4((float)inp[base + j], (float)inp[base + j + TG],\n                      (float)inp[base + j + TG*2], (float)inp[base + j + TG*3]);\n    \n    v = v * inv * w;\n    \n    out[base + j] = (T)v.x;\n    out[base + j + TG] = (T)v.y;\n    out[base + j + TG*2] = (T)v.z;\n    out[base + j + TG*3] = (T)v.w;\n}",
    "gen6_59e78975e6ce": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint CHUNK = 512;  // Process in 512-element chunks\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Process in chunks for better cache locality\nfloat sumsq = 0.0f;\n\nfor (uint chunk_start = 0; chunk_start < D; chunk_start += CHUNK) {\n    uint chunk_end = min(chunk_start + CHUNK, D);\n    \n    for (uint j = chunk_start + tid; j < chunk_end; j += TG) {\n        float v = (float)inp[base + j];\n        sumsq += v * v;\n    }\n}\n\n// SIMD reduction\nsumsq = simd_sum(sumsq);\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float final_sum = buf[tid];\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Write back with chunked access\nfor (uint chunk_start = 0; chunk_start < D; chunk_start += CHUNK) {\n    uint chunk_end = min(chunk_start + CHUNK, D);\n    \n    for (uint j = chunk_start + tid; j < chunk_end; j += TG) {\n        out[base + j] = (T)((float)inp[base + j] * inv * (float)weight[j]);\n    }\n}",
    "gen6_6b12647487b2": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\n// Phase 1: Compute sum-of-squares with 8x unroll\nfloat sumsq = 0.0f;\n\nfor (uint j = tid; j < D; j += TG * 8) {\n    uint idx = base + j;\n    float v0 = (float)inp[idx];\n    float v1 = (float)inp[idx + TG];\n    float v2 = (float)inp[idx + TG*2];\n    float v3 = (float)inp[idx + TG*3];\n    float v4 = (float)inp[idx + TG*4];\n    float v5 = (float)inp[idx + TG*5];\n    float v6 = (float)inp[idx + TG*6];\n    float v7 = (float)inp[idx + TG*7];\n    \n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3 + v4*v4 + v5*v5 + v6*v6 + v7*v7;\n}\n\n// SIMD-level reduction\nsumsq = simd_sum(sumsq);\n\n// Compact cross-SIMD reduction using shuffle\nthreadgroup float shared_sum;\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    float group_sum = sumsq;\n    \n    // Manual reduction across 8 SIMD groups using atomic operations\n    if (simd_group == 0) {\n        shared_sum = 0.0f;\n    }\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    \n    // Each SIMD group leader adds its contribution\n    threadgroup atomic_float* atomic_sum = (threadgroup atomic_float*)&shared_sum;\n    atomic_fetch_add_explicit(atomic_sum, group_sum, memory_order_relaxed);\n}\n\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(shared_sum / (float)D + EPS);\n\n// Phase 2: Fused multiply with 8x unroll\nfor (uint j = tid; j < D; j += TG * 8) {\n    uint idx = base + j;\n    \n    out[idx] = (T)((float)inp[idx] * inv * (float)weight[j]);\n    out[idx + TG] = (T)((float)inp[idx + TG] * inv * (float)weight[j + TG]);\n    out[idx + TG*2] = (T)((float)inp[idx + TG*2] * inv * (float)weight[j + TG*2]);\n    out[idx + TG*3] = (T)((float)inp[idx + TG*3] * inv * (float)weight[j + TG*3]);\n    out[idx + TG*4] = (T)((float)inp[idx + TG*4] * inv * (float)weight[j + TG*4]);\n    out[idx + TG*5] = (T)((float)inp[idx + TG*5] * inv * (float)weight[j + TG*5]);\n    out[idx + TG*6] = (T)((float)inp[idx + TG*6] * inv * (float)weight[j + TG*6]);\n    out[idx + TG*7] = (T)((float)inp[idx + TG*7] * inv * (float)weight[j + TG*7]);\n}",
    "gen6_4ef1db3a0f7a": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Compute sum-of-squares with 4x unroll\nfloat sumsq = 0.0f;\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    \n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\n// SIMD reduction\nsumsq = simd_sum(sumsq);\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\n\nif (simd_lane == 0) {\n    buf[simd_group] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid < 8) {\n    float final_sum = buf[tid];\n    final_sum = simd_sum(final_sum);\n    if (tid == 0) {\n        buf[0] = final_sum;\n    }\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\n\n// Phase 2: Prefetch and compute with register rotation\nfloat inp_reg[4];\nfloat weight_reg[4];\n\n// Prefetch first batch\nfor (uint k = 0; k < 4; k++) {\n    uint idx = tid + k * TG;\n    if (idx < D) {\n        inp_reg[k] = (float)inp[base + idx];\n        weight_reg[k] = (float)weight[idx];\n    }\n}\n\nfor (uint j = tid; j < D; j += TG * 4) {\n    // Compute using prefetched values\n    float result[4];\n    for (uint k = 0; k < 4; k++) {\n        result[k] = inp_reg[k] * inv * weight_reg[k];\n    }\n    \n    // Prefetch next batch while computing\n    uint next_j = j + TG * 4;\n    for (uint k = 0; k < 4; k++) {\n        uint idx = next_j + k * TG;\n        if (idx < D) {\n            inp_reg[k] = (float)inp[base + idx];\n            weight_reg[k] = (float)weight[idx];\n        }\n    }\n    \n    // Write results\n    out[base + j] = (T)result[0];\n    out[base + j + TG] = (T)result[1];\n    out[base + j + TG*2] = (T)result[2];\n    out[base + j + TG*3] = (T)result[3];\n}",
    "gen7_bd69b0335d15": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Reduction with float2 vectorization\nfloat sumsq = 0.0f;\nfor (uint j = tid * 2; j < D; j += TG * 8) {\n    float2 v0 = float2((float)inp[base + j], (float)inp[base + j + 1]);\n    float2 v1 = float2((float)inp[base + j + TG*2], (float)inp[base + j + TG*2 + 1]);\n    float2 v2 = float2((float)inp[base + j + TG*4], (float)inp[base + j + TG*4 + 1]);\n    float2 v3 = float2((float)inp[base + j + TG*6], (float)inp[base + j + TG*6 + 1]);\n    sumsq += v0.x*v0.x + v0.y*v0.y + v1.x*v1.x + v1.y*v1.y + v2.x*v2.x + v2.y*v2.y + v3.x*v3.x + v3.y*v3.y;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: float2 vectorized output\nfor (uint j = tid * 2; j < D; j += TG * 8) {\n    float2 in0 = float2((float)inp[base + j], (float)inp[base + j + 1]);\n    float2 in1 = float2((float)inp[base + j + TG*2], (float)inp[base + j + TG*2 + 1]);\n    float2 in2 = float2((float)inp[base + j + TG*4], (float)inp[base + j + TG*4 + 1]);\n    float2 in3 = float2((float)inp[base + j + TG*6], (float)inp[base + j + TG*6 + 1]);\n    \n    float2 w0 = float2((float)weight[j], (float)weight[j + 1]);\n    float2 w1 = float2((float)weight[j + TG*2], (float)weight[j + TG*2 + 1]);\n    float2 w2 = float2((float)weight[j + TG*4], (float)weight[j + TG*4 + 1]);\n    float2 w3 = float2((float)weight[j + TG*6], (float)weight[j + TG*6 + 1]);\n    \n    out[base + j] = (T)(in0.x * inv * w0.x);\n    out[base + j + 1] = (T)(in0.y * inv * w0.y);\n    out[base + j + TG*2] = (T)(in1.x * inv * w1.x);\n    out[base + j + TG*2 + 1] = (T)(in1.y * inv * w1.y);\n    out[base + j + TG*4] = (T)(in2.x * inv * w2.x);\n    out[base + j + TG*4 + 1] = (T)(in2.y * inv * w2.y);\n    out[base + j + TG*6] = (T)(in3.x * inv * w3.x);\n    out[base + j + TG*6 + 1] = (T)(in3.y * inv * w3.y);\n}",
    "gen7_42df2faca8f4": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\nthreadgroup float cached_weights[D];\n\n// Phase 1: Reduction while caching weights\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n    cached_weights[j] = (float)weight[j];\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Use cached weights (no global weight loads)\nfor (uint j = tid; j < D; j += TG * 4) {\n    float in0 = (float)inp[base + j];\n    float in1 = (float)inp[base + j + TG];\n    float in2 = (float)inp[base + j + TG*2];\n    float in3 = (float)inp[base + j + TG*3];\n    \n    float w0 = cached_weights[j];\n    float w1 = cached_weights[j + TG];\n    float w2 = cached_weights[j + TG*2];\n    float w3 = cached_weights[j + TG*3];\n    \n    out[base + j] = (T)(in0 * inv * w0);\n    out[base + j + TG] = (T)(in1 * inv * w1);\n    out[base + j + TG*2] = (T)(in2 * inv * w2);\n    out[base + j + TG*3] = (T)(in3 * inv * w3);\n}",
    "gen7_a92920dd510a": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: 6-way unrolled reduction\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 6) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    float v4 = (float)inp[base + j + TG*4];\n    float v5 = (float)inp[base + j + TG*5];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3 + v4*v4 + v5*v5;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: 6-way unrolled computation\nfor (uint j = tid; j < D; j += TG * 6) {\n    float in0 = (float)inp[base + j];\n    float in1 = (float)inp[base + j + TG];\n    float in2 = (float)inp[base + j + TG*2];\n    float in3 = (float)inp[base + j + TG*3];\n    float in4 = (float)inp[base + j + TG*4];\n    float in5 = (float)inp[base + j + TG*5];\n    \n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    float w4 = (float)weight[j + TG*4];\n    float w5 = (float)weight[j + TG*5];\n    \n    out[base + j] = (T)(in0 * inv * w0);\n    out[base + j + TG] = (T)(in1 * inv * w1);\n    out[base + j + TG*2] = (T)(in2 * inv * w2);\n    out[base + j + TG*3] = (T)(in3 * inv * w3);\n    out[base + j + TG*4] = (T)(in4 * inv * w4);\n    out[base + j + TG*5] = (T)(in5 * inv * w5);\n}",
    "gen7_d584e9c4a843": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[8];\n\n// Phase 1: Reduction with SIMD-only path\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG * 4) {\n    float v0 = (float)inp[base + j];\n    float v1 = (float)inp[base + j + TG];\n    float v2 = (float)inp[base + j + TG*2];\n    float v3 = (float)inp[base + j + TG*3];\n    sumsq += v0*v0 + v1*v1 + v2*v2 + v3*v3;\n}\n\n// SIMD-level reduction (within each warp of 32)\nfloat simd_sum = simd_sum(sumsq);\n\n// Only one thread per SIMD group writes to shared memory\nuint simd_lane = tid % 32;\nuint simd_group = tid / 32;\nif (simd_lane == 0) {\n    buf[simd_group] = simd_sum;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction of 8 values (256/32 = 8 SIMD groups)\nfloat total = 0.0f;\nif (tid < 8) {\n    total = buf[tid];\n}\ntotal = simd_sum(total);\nfloat inv = metal::rsqrt(total / (float)D + EPS);\n\n// Phase 2: Standard computation\nfor (uint j = tid; j < D; j += TG * 4) {\n    uint idx0 = base + j;\n    uint idx1 = idx0 + TG;\n    uint idx2 = idx0 + TG*2;\n    uint idx3 = idx0 + TG*3;\n    \n    float in0 = (float)inp[idx0];\n    float in1 = (float)inp[idx1];\n    float in2 = (float)inp[idx2];\n    float in3 = (float)inp[idx3];\n    \n    float w0 = (float)weight[j];\n    float w1 = (float)weight[j + TG];\n    float w2 = (float)weight[j + TG*2];\n    float w3 = (float)weight[j + TG*3];\n    \n    out[idx0] = (T)(in0 * inv * w0);\n    out[idx1] = (T)(in1 * inv * w1);\n    out[idx2] = (T)(in2 * inv * w2);\n    out[idx3] = (T)(in3 * inv * w3);\n}",
    "gen8_0289c06b333b": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: float4 vectorized reduction (D=2048, 4 elements/load, TG=256 \u2192 2 iterations)\nfloat sumsq = 0.0f;\nuint vec_stride = TG * 4;\n\nfor (uint j = tid * 4; j < D; j += vec_stride) {\n    uint idx = base + j;\n    float4 v = float4(inp[idx], inp[idx+1], inp[idx+2], inp[idx+3]);\n    sumsq += dot(v, v);\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: float4 vectorized output\nfor (uint j = tid * 4; j < D; j += vec_stride) {\n    uint idx = base + j;\n    uint w_idx = j;\n    \n    float4 in_vec = float4(inp[idx], inp[idx+1], inp[idx+2], inp[idx+3]);\n    float4 w_vec = float4(weight[w_idx], weight[w_idx+1], weight[w_idx+2], weight[w_idx+3]);\n    float4 result = in_vec * inv * w_vec;\n    \n    out[idx] = (T)result.x;\n    out[idx+1] = (T)result.y;\n    out[idx+2] = (T)result.z;\n    out[idx+3] = (T)result.w;\n}",
    "gen8_f1c8018566ed": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint PARTIALS = 16;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\n\n// Phase 1: Accumulate into 16 register bins to maximize FMA throughput\nfloat partial[PARTIALS];\nfor (uint p = 0; p < PARTIALS; ++p) partial[p] = 0.0f;\n\nuint elements_per_thread = D / TG; // 2048/256 = 8\nuint start = tid * elements_per_thread;\n\nfor (uint i = 0; i < elements_per_thread; ++i) {\n    uint idx = base + start + i;\n    float v = (float)inp[idx];\n    partial[i % PARTIALS] += v * v;\n}\n\n// Tree reduce the partials in registers\nfloat sumsq = 0.0f;\nfor (uint p = 0; p < PARTIALS; ++p) sumsq += partial[p];\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Coalesced output with direct mapping\nfor (uint i = 0; i < elements_per_thread; ++i) {\n    uint idx = base + start + i;\n    uint w_idx = start + i;\n    out[idx] = (T)((float)inp[idx] * inv * (float)weight[w_idx]);\n}",
    "gen8_6686a9fea8c6": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[32]; // Only 32 slots for warp leaders\n\n// Phase 1: Each thread accumulates 8 values\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\n// SIMD-level reduction within each warp (32 threads)\nsumsq = simd_sum(sumsq);\n\n// Only lane 0 of each warp writes to shared memory\nuint warp_id = tid / 32;\nuint lane_id = tid % 32;\nif (lane_id == 0) {\n    buf[warp_id] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Final reduction by first warp only\nfloat final_sum = 0.0f;\nif (tid < 32) {\n    final_sum = (tid < 8) ? buf[tid] : 0.0f;\n    final_sum = simd_sum(final_sum);\n}\n\nfloat inv;\nif (tid == 0) {\n    buf[0] = metal::rsqrt(final_sum / (float)D + EPS);\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\ninv = buf[0];\n\n// Phase 2: Coalesced output\nfor (uint j = tid; j < D; j += TG) {\n    uint idx = base + j;\n    out[idx] = (T)((float)inp[idx] * inv * (float)weight[j]);\n}",
    "gen8_578bdbdd7950": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint TILE_SIZE = 256;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float buf[TG];\nthreadgroup float weight_cache[TILE_SIZE];\n\n// Phase 1: Standard reduction\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 2: Tile-based computation with cached weights\nuint num_tiles = D / TILE_SIZE; // 2048/256 = 8 tiles\n\nfor (uint tile = 0; tile < num_tiles; ++tile) {\n    uint tile_base = tile * TILE_SIZE;\n    \n    // Collaboratively load weight tile\n    weight_cache[tid] = (float)weight[tile_base + tid];\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    \n    // Compute outputs using cached weights\n    uint idx = base + tile_base + tid;\n    out[idx] = (T)((float)inp[idx] * inv * weight_cache[tid]);\n    \n    threadgroup_barrier(mem_flags::mem_threadgroup);\n}",
    "gen9_5563d1868dd8": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint CHUNK = 16;  // 64-byte cache line\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float sumsq_buf[TG];\nthreadgroup float val_buf[D];\n\n// Load and reduce in cache-line-sized chunks\nfloat sumsq = 0.0f;\nfor (uint chunk_base = 0; chunk_base < D; chunk_base += TG * CHUNK) {\n    uint idx = chunk_base + tid * CHUNK;\n    if (idx + CHUNK <= D) {\n        // Aligned 64-byte chunk read\n        for (uint k = 0; k < CHUNK; k++) {\n            float v = (float)inp[base + idx + k];\n            val_buf[idx + k] = v;\n            sumsq += v * v;\n        }\n    } else {\n        // Handle remainder\n        for (uint k = 0; k < CHUNK && idx + k < D; k++) {\n            float v = (float)inp[base + idx + k];\n            val_buf[idx + k] = v;\n            sumsq += v * v;\n        }\n    }\n}\n\nKK_SIMD_REDUCE_SUM(sumsq_buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(sumsq_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Write in cache-line chunks\nfor (uint chunk_base = 0; chunk_base < D; chunk_base += TG * CHUNK) {\n    uint idx = chunk_base + tid * CHUNK;\n    if (idx + CHUNK <= D) {\n        for (uint k = 0; k < CHUNK; k++) {\n            out[base + idx + k] = (T)(val_buf[idx + k] * inv * (float)weight[idx + k]);\n        }\n    } else {\n        for (uint k = 0; k < CHUNK && idx + k < D; k++) {\n            out[base + idx + k] = (T)(val_buf[idx + k] * inv * (float)weight[idx + k]);\n        }\n    }\n}",
    "gen9_c1b180269a14": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint lane_id = tid & 31;\nuint warp_id = tid >> 5;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float warp_sums[8];  // 8 warps in TG=256\nthreadgroup float val_buf[D];\n\n// Phase 1: Load and compute per-thread sum\nfloat sumsq = 0.0f;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    val_buf[j] = v;\n    sumsq += v * v;\n}\n\n// Phase 2: SIMD-level reduction (32 threads per warp)\nfor (uint offset = 16; offset > 0; offset >>= 1) {\n    sumsq += simd_shuffle_down(sumsq, offset);\n}\n\n// Phase 3: Warp leaders write to shared memory\nif (lane_id == 0) {\n    warp_sums[warp_id] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Phase 4: Final reduction by first warp\nfloat total_sumsq = 0.0f;\nif (tid < 8) {\n    total_sumsq = warp_sums[tid];\n    for (uint offset = 4; offset > 0; offset >>= 1) {\n        total_sumsq += simd_shuffle_down(total_sumsq, offset);\n    }\n}\n\nthreadgroup_barrier(mem_flags::mem_threadgroup);\nfloat inv = metal::rsqrt(total_sumsq / (float)D + EPS);\n\n// Phase 5: Compute output\nfor (uint j = tid; j < D; j += TG) {\n    out[base + j] = (T)(val_buf[j] * inv * (float)weight[j]);\n}",
    "gen9_f81f90f5a6a3": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;  // 8\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float final_sum[1];\nfloat local_vals[ELEMS_PER_THREAD];\nfloat local_weights[ELEMS_PER_THREAD];\n\n// Level 1: Register accumulation (8 elements per thread)\nfloat sumsq = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    local_vals[k] = (float)inp[base + offset + k];\n    local_weights[k] = (float)weight[offset + k];\n    sumsq += local_vals[k] * local_vals[k];\n}\n\n// Level 2: SIMD reduction (32 lanes)\nfor (uint s = 16; s > 0; s >>= 1) {\n    sumsq += simd_shuffle_down(sumsq, s);\n}\n\n// Level 3: Threadgroup reduction (8 SIMD groups)\nthreadgroup float simd_results[TG / 32];\nuint lane = tid & 31;\nuint simd_idx = tid >> 5;\n\nif (lane == 0) {\n    simd_results[simd_idx] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid == 0) {\n    float total = 0.0f;\n    for (uint i = 0; i < TG / 32; i++) {\n        total += simd_results[i];\n    }\n    final_sum[0] = total;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = metal::rsqrt(final_sum[0] / (float)D + EPS);\n\n// Direct register output\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(local_vals[k] * inv * local_weights[k]);\n}",
    "gen9_d505e8fc5f94": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float sumsq_buf[TG];\nthreadgroup float val_buf[D];\nfloat weight_cache[8];  // Register cache for weights\n\n// Fused load: input values, weights, and compute sumsq\nfloat sumsq = 0.0f;\nuint elem_count = 0;\nfor (uint j = tid; j < D; j += TG) {\n    float v = (float)inp[base + j];\n    float w = (float)weight[j];\n    \n    val_buf[j] = v;\n    \n    // Cache weight in registers for reuse\n    if (elem_count < 8) {\n        weight_cache[elem_count] = w;\n    }\n    elem_count++;\n    \n    sumsq += v * v;\n}\n\nKK_SIMD_REDUCE_SUM(sumsq_buf, sumsq, tid, TG);\nfloat inv = metal::rsqrt(sumsq_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Output phase: use cached weights when available\nelem_count = 0;\nfor (uint j = tid; j < D; j += TG) {\n    float w = (elem_count < 8) ? weight_cache[elem_count] : (float)weight[j];\n    out[base + j] = (T)(val_buf[j] * inv * w);\n    elem_count++;\n}",
    "gen10_fa94523d84f7": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float simd_partials[8];\nfloat local_vals[ELEMS_PER_THREAD];\nfloat local_weights[ELEMS_PER_THREAD];\n\n// Register accumulation\nfloat sumsq = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    local_vals[k] = (float)inp[base + offset + k];\n    local_weights[k] = (float)weight[offset + k];\n    sumsq += local_vals[k] * local_vals[k];\n}\n\n// SIMD reduction\nsumsq += simd_shuffle_down(sumsq, 16);\nsumsq += simd_shuffle_down(sumsq, 8);\nsumsq += simd_shuffle_down(sumsq, 4);\nsumsq += simd_shuffle_down(sumsq, 2);\nsumsq += simd_shuffle_down(sumsq, 1);\n\nuint lane = tid & 31;\nuint simd_idx = tid >> 5;\n\n// Lane 0 of each warp writes partial\nif (lane == 0) {\n    simd_partials[simd_idx] = sumsq;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// First warp does final reduction and broadcasts\nfloat inv;\nif (tid < 8) {\n    float total = simd_partials[tid];\n    total += simd_shuffle_down(total, 4);\n    total += simd_shuffle_down(total, 2);\n    total += simd_shuffle_down(total, 1);\n    inv = metal::rsqrt(total / (float)D + EPS);\n    simd_partials[0] = inv;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\ninv = simd_partials[0];\n\n// Unrolled output with explicit FMA\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(local_vals[k] * inv * local_weights[k]);\n}",
    "gen10_ccc9b4566b84": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float shared_inv[1];\nfloat vals[ELEMS_PER_THREAD];\nfloat wts[ELEMS_PER_THREAD];\n\n// Load and accumulate\nfloat sumsq = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\n#pragma unroll\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    vals[k] = (float)inp[base + offset + k];\n    wts[k] = (float)weight[offset + k];\n    sumsq = metal::fma(vals[k], vals[k], sumsq);\n}\n\n// Warp reduction\nfor (uint s = 16; s > 0; s >>= 1) {\n    sumsq += simd_shuffle_down(sumsq, s);\n}\n\n// Threadgroup reduction\nthreadgroup float warp_sums[8];\nuint lane = tid & 31;\nuint warp_id = tid >> 5;\n\nif (lane == 0) warp_sums[warp_id] = sumsq;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nif (tid == 0) {\n    float total = warp_sums[0] + warp_sums[1] + warp_sums[2] + warp_sums[3] +\n                  warp_sums[4] + warp_sums[5] + warp_sums[6] + warp_sums[7];\n    shared_inv[0] = metal::rsqrt(total / (float)D + EPS);\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv = shared_inv[0];\n\n// Fused multiply-add chain: inv*weight then result*val\n#pragma unroll\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    float scale = inv * wts[k];\n    out[base + offset + k] = (T)(vals[k] * scale);\n}",
    "gen10_524764fd5e17": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduction_buffer[8];\nfloat vals[ELEMS_PER_THREAD];\nfloat weights[ELEMS_PER_THREAD];\n\n// Dual accumulator for odd/even elements\nfloat sumsq_even = 0.0f;\nfloat sumsq_odd = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\n\n// Process pairs to exploit dual-issue\nfor (uint k = 0; k < ELEMS_PER_THREAD; k += 2) {\n    vals[k] = (float)inp[base + offset + k];\n    vals[k+1] = (float)inp[base + offset + k + 1];\n    weights[k] = (float)weight[offset + k];\n    weights[k+1] = (float)weight[offset + k + 1];\n    sumsq_even += vals[k] * vals[k];\n    sumsq_odd += vals[k+1] * vals[k+1];\n}\n\nfloat sumsq = sumsq_even + sumsq_odd;\n\n// Butterfly SIMD reduction\nsumsq += simd_shuffle_xor(sumsq, 16);\nsumsq += simd_shuffle_xor(sumsq, 8);\nsumsq += simd_shuffle_xor(sumsq, 4);\nsumsq += simd_shuffle_xor(sumsq, 2);\nsumsq += simd_shuffle_xor(sumsq, 1);\n\nuint lane = tid & 31;\nuint warp_id = tid >> 5;\n\nif (lane == 0) reduction_buffer[warp_id] = sumsq;\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat inv;\nif (tid == 0) {\n    float total = 0.0f;\n    for (uint i = 0; i < 8; i++) total += reduction_buffer[i];\n    inv = metal::rsqrt(total / (float)D + EPS);\n    reduction_buffer[0] = inv;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\ninv = reduction_buffer[0];\n\n// Pipelined stores with register rotation\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(vals[k] * inv * weights[k]);\n}",
    "gen10_f505fd4fe1c9": "constexpr uint D = 2048;\nconstexpr uint TG = 256;\nconstexpr float EPS = 1e-6f;\nconstexpr uint ELEMS_PER_THREAD = D / TG;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\n// Minimal shared memory - single cache line\nthreadgroup float shared[16];\nfloat regs[ELEMS_PER_THREAD];\nfloat wt_regs[ELEMS_PER_THREAD];\n\n// Accumulate in registers\nfloat acc = 0.0f;\nuint offset = tid * ELEMS_PER_THREAD;\n\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    regs[k] = (float)inp[base + offset + k];\n    wt_regs[k] = (float)weight[offset + k];\n    acc += regs[k] * regs[k];\n}\n\n// Full SIMD reduction (32\u21921)\nacc += simd_shuffle_down(acc, 16);\nacc += simd_shuffle_down(acc, 8);\nacc += simd_shuffle_down(acc, 4);\nacc += simd_shuffle_down(acc, 2);\nacc += simd_shuffle_down(acc, 1);\n\nuint lane = tid & 31;\nuint warp = tid >> 5;\n\n// Only 8 warps write\nif (lane == 0 && warp < 8) {\n    shared[warp] = acc;\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\n// Single thread final reduction\nif (tid == 0) {\n    float sum = shared[0] + shared[1] + shared[2] + shared[3] +\n                shared[4] + shared[5] + shared[6] + shared[7];\n    shared[0] = metal::rsqrt(sum / (float)D + EPS);\n}\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfloat scale = shared[0];\n\n// Compute and store with compiler-friendly loop\nfor (uint k = 0; k < ELEMS_PER_THREAD; k++) {\n    out[base + offset + k] = (T)(regs[k] * scale * wt_regs[k]);\n}"
  }
}