{
  "schema_version": "2",
  "runtime": {
    "mlx_version": "0.30.6.dev20260208+185b06d9",
    "device_name": "Apple M4 Max",
    "device_arch": "applegpu_g16s"
  },
  "entries": [
    {
      "key": {
        "op_name": "rmsnorm_residual",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 128,
          "D": 2048
        }
      },
      "candidate_id": "rmsnorm_residual_3d9e4da0b8a43ea6",
      "func_name": "kk_kd_rmsnorm_res_d2048_tg128_v2_u2_simd0",
      "metal_source": "constexpr uint D = 2048;\nconstexpr uint TG = 128;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 2;\nconstexpr float EPS = 1e-06f;\nconstexpr bool USE_SIMD = false;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduce_buf[TG];\n\nfloat sumsq = 0.0f;\nuint start = tid * VEC;\nuint step = TG * VEC;\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float x = (float)inp[base + idx] + (float)residual[base + idx];\n                updated_res[base + idx] = (T)x;\n                sumsq += x * x;\n            }\n        }\n    }\n}\n\nif (USE_SIMD) {\n    KK_SIMD_REDUCE_SUM(reduce_buf, sumsq, tid, TG);\n} else {\n    reduce_buf[tid] = sumsq;\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint stride = TG / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            reduce_buf[tid] += reduce_buf[tid + stride];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n}\n\nfloat inv = metal::rsqrt(reduce_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float vres = (float)updated_res[base + idx];\n                float w = (float)weight[idx];\n                out[base + idx] = (T)(vres * inv * w);\n            }\n        }\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "inp",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "residual",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "weight",
          "dtype": "float16",
          "shape": [
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "updated_res",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 2,
        "use_simd": false,
        "eps": 1e-06
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "rmsnorm_residual_rows_tg"
      },
      "source_hash": "4bd7b50bdc2ce461106b03132bde470933a6e22b57c4f59427ccc5cb3e6b2293",
      "metrics": {
        "latency_us": 160.1875,
        "speedup_vs_ref": 1.9301599687865783,
        "correctness_max_abs_err": 0.0078125,
        "correctness_max_rel_err": 0.0012755102040816326
      }
    },
    {
      "key": {
        "op_name": "rmsnorm_residual",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 256,
          "D": 2048
        }
      },
      "candidate_id": "rmsnorm_residual_3d9e4da0b8a43ea6",
      "func_name": "kk_kd_rmsnorm_res_d2048_tg128_v2_u2_simd0",
      "metal_source": "constexpr uint D = 2048;\nconstexpr uint TG = 128;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 2;\nconstexpr float EPS = 1e-06f;\nconstexpr bool USE_SIMD = false;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduce_buf[TG];\n\nfloat sumsq = 0.0f;\nuint start = tid * VEC;\nuint step = TG * VEC;\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float x = (float)inp[base + idx] + (float)residual[base + idx];\n                updated_res[base + idx] = (T)x;\n                sumsq += x * x;\n            }\n        }\n    }\n}\n\nif (USE_SIMD) {\n    KK_SIMD_REDUCE_SUM(reduce_buf, sumsq, tid, TG);\n} else {\n    reduce_buf[tid] = sumsq;\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint stride = TG / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            reduce_buf[tid] += reduce_buf[tid + stride];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n}\n\nfloat inv = metal::rsqrt(reduce_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float vres = (float)updated_res[base + idx];\n                float w = (float)weight[idx];\n                out[base + idx] = (T)(vres * inv * w);\n            }\n        }\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "inp",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "residual",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "weight",
          "dtype": "float16",
          "shape": [
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "updated_res",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 2,
        "use_simd": false,
        "eps": 1e-06
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "rmsnorm_residual_rows_tg"
      },
      "source_hash": "4bd7b50bdc2ce461106b03132bde470933a6e22b57c4f59427ccc5cb3e6b2293",
      "metrics": {
        "latency_us": 160.1875,
        "speedup_vs_ref": 1.9301599687865783,
        "correctness_max_abs_err": 0.0078125,
        "correctness_max_rel_err": 0.0012755102040816326
      }
    },
    {
      "key": {
        "op_name": "rmsnorm_residual",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 512,
          "D": 2048
        }
      },
      "candidate_id": "rmsnorm_residual_3d9e4da0b8a43ea6",
      "func_name": "kk_kd_rmsnorm_res_d2048_tg128_v2_u2_simd0",
      "metal_source": "constexpr uint D = 2048;\nconstexpr uint TG = 128;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 2;\nconstexpr float EPS = 1e-06f;\nconstexpr bool USE_SIMD = false;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduce_buf[TG];\n\nfloat sumsq = 0.0f;\nuint start = tid * VEC;\nuint step = TG * VEC;\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float x = (float)inp[base + idx] + (float)residual[base + idx];\n                updated_res[base + idx] = (T)x;\n                sumsq += x * x;\n            }\n        }\n    }\n}\n\nif (USE_SIMD) {\n    KK_SIMD_REDUCE_SUM(reduce_buf, sumsq, tid, TG);\n} else {\n    reduce_buf[tid] = sumsq;\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint stride = TG / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            reduce_buf[tid] += reduce_buf[tid + stride];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n}\n\nfloat inv = metal::rsqrt(reduce_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float vres = (float)updated_res[base + idx];\n                float w = (float)weight[idx];\n                out[base + idx] = (T)(vres * inv * w);\n            }\n        }\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "inp",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "residual",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "weight",
          "dtype": "float16",
          "shape": [
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "updated_res",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 2,
        "use_simd": false,
        "eps": 1e-06
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "rmsnorm_residual_rows_tg"
      },
      "source_hash": "4bd7b50bdc2ce461106b03132bde470933a6e22b57c4f59427ccc5cb3e6b2293",
      "metrics": {
        "latency_us": 160.1875,
        "speedup_vs_ref": 1.9301599687865783,
        "correctness_max_abs_err": 0.0078125,
        "correctness_max_rel_err": 0.0012755102040816326
      }
    },
    {
      "key": {
        "op_name": "rmsnorm_residual",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 1024,
          "D": 2048
        }
      },
      "candidate_id": "rmsnorm_residual_3d9e4da0b8a43ea6",
      "func_name": "kk_kd_rmsnorm_res_d2048_tg128_v2_u2_simd0",
      "metal_source": "constexpr uint D = 2048;\nconstexpr uint TG = 128;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 2;\nconstexpr float EPS = 1e-06f;\nconstexpr bool USE_SIMD = false;\n\nuint gid = thread_position_in_grid.x;\nuint tid = thread_position_in_threadgroup.x;\nuint row = gid / TG;\nuint base = row * D;\n\nthreadgroup float reduce_buf[TG];\n\nfloat sumsq = 0.0f;\nuint start = tid * VEC;\nuint step = TG * VEC;\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float x = (float)inp[base + idx] + (float)residual[base + idx];\n                updated_res[base + idx] = (T)x;\n                sumsq += x * x;\n            }\n        }\n    }\n}\n\nif (USE_SIMD) {\n    KK_SIMD_REDUCE_SUM(reduce_buf, sumsq, tid, TG);\n} else {\n    reduce_buf[tid] = sumsq;\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint stride = TG / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            reduce_buf[tid] += reduce_buf[tid + stride];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n}\n\nfloat inv = metal::rsqrt(reduce_buf[0] / (float)D + EPS);\nthreadgroup_barrier(mem_flags::mem_threadgroup);\n\nfor (uint j0 = start; j0 < D; j0 += step * UNROLL) {\n    #pragma unroll\n    for (uint u = 0; u < UNROLL; ++u) {\n        uint j = j0 + u * step;\n        if (j >= D) {\n            continue;\n        }\n        #pragma unroll\n        for (uint v = 0; v < VEC; ++v) {\n            uint idx = j + v;\n            if (idx < D) {\n                float vres = (float)updated_res[base + idx];\n                float w = (float)weight[idx];\n                out[base + idx] = (T)(vres * inv * w);\n            }\n        }\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "inp",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "residual",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "weight",
          "dtype": "float16",
          "shape": [
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        },
        {
          "name": "updated_res",
          "dtype": "float16",
          "shape": [
            128,
            2048
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 2,
        "use_simd": false,
        "eps": 1e-06
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "rmsnorm_residual_rows_tg"
      },
      "source_hash": "4bd7b50bdc2ce461106b03132bde470933a6e22b57c4f59427ccc5cb3e6b2293",
      "metrics": {
        "latency_us": 160.1875,
        "speedup_vs_ref": 1.9301599687865783,
        "correctness_max_abs_err": 0.0078125,
        "correctness_max_rel_err": 0.0012755102040816326
      }
    },
    {
      "key": {
        "op_name": "swiglu",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 128,
          "D": 1536,
          "N": 196608
        }
      },
      "candidate_id": "swiglu_e79b57774f9d99c2",
      "func_name": "kk_kd_swiglu_n196608_tg128_v2_u1_fast1",
      "metal_source": "constexpr uint N = 196608;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr bool FAST_SIGMOID = true;\n\nuint tid = thread_position_in_grid.x;\nuint base = tid * VEC * UNROLL;\n\n#pragma unroll\nfor (uint u = 0; u < UNROLL; ++u) {\n    uint idx0 = base + u * VEC;\n    if (idx0 >= N) {\n        continue;\n    }\n\n    #pragma unroll\n    for (uint v = 0; v < VEC; ++v) {\n        uint idx = idx0 + v;\n        if (idx >= N) {\n            continue;\n        }\n\n        float g = (float)gate[idx];\n        float upv = (float)up[idx];\n        float sig;\n        if (FAST_SIGMOID) {\n            sig = kk_sigmoid(g);\n        } else {\n            sig = 1.0f / (1.0f + metal::exp(-g));\n        }\n        out[idx] = (T)(g * sig * upv);\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "gate",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        },
        {
          "name": "up",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "fast_sigmoid": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "swiglu_flat",
        "vec_width": 2,
        "unroll": 1
      },
      "source_hash": "299fcfaa43eab15032de60da82a02a263704893647cda1d2cc0b2111b920c300",
      "metrics": {
        "latency_us": 195.6455,
        "speedup_vs_ref": 1.2776092473376592,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "swiglu",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 256,
          "D": 1536,
          "N": 393216
        }
      },
      "candidate_id": "swiglu_e79b57774f9d99c2",
      "func_name": "kk_kd_swiglu_n196608_tg128_v2_u1_fast1",
      "metal_source": "constexpr uint N = 196608;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr bool FAST_SIGMOID = true;\n\nuint tid = thread_position_in_grid.x;\nuint base = tid * VEC * UNROLL;\n\n#pragma unroll\nfor (uint u = 0; u < UNROLL; ++u) {\n    uint idx0 = base + u * VEC;\n    if (idx0 >= N) {\n        continue;\n    }\n\n    #pragma unroll\n    for (uint v = 0; v < VEC; ++v) {\n        uint idx = idx0 + v;\n        if (idx >= N) {\n            continue;\n        }\n\n        float g = (float)gate[idx];\n        float upv = (float)up[idx];\n        float sig;\n        if (FAST_SIGMOID) {\n            sig = kk_sigmoid(g);\n        } else {\n            sig = 1.0f / (1.0f + metal::exp(-g));\n        }\n        out[idx] = (T)(g * sig * upv);\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "gate",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        },
        {
          "name": "up",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "fast_sigmoid": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "swiglu_flat",
        "vec_width": 2,
        "unroll": 1
      },
      "source_hash": "299fcfaa43eab15032de60da82a02a263704893647cda1d2cc0b2111b920c300",
      "metrics": {
        "latency_us": 195.6455,
        "speedup_vs_ref": 1.2776092473376592,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "swiglu",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 512,
          "D": 1536,
          "N": 786432
        }
      },
      "candidate_id": "swiglu_e79b57774f9d99c2",
      "func_name": "kk_kd_swiglu_n196608_tg128_v2_u1_fast1",
      "metal_source": "constexpr uint N = 196608;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr bool FAST_SIGMOID = true;\n\nuint tid = thread_position_in_grid.x;\nuint base = tid * VEC * UNROLL;\n\n#pragma unroll\nfor (uint u = 0; u < UNROLL; ++u) {\n    uint idx0 = base + u * VEC;\n    if (idx0 >= N) {\n        continue;\n    }\n\n    #pragma unroll\n    for (uint v = 0; v < VEC; ++v) {\n        uint idx = idx0 + v;\n        if (idx >= N) {\n            continue;\n        }\n\n        float g = (float)gate[idx];\n        float upv = (float)up[idx];\n        float sig;\n        if (FAST_SIGMOID) {\n            sig = kk_sigmoid(g);\n        } else {\n            sig = 1.0f / (1.0f + metal::exp(-g));\n        }\n        out[idx] = (T)(g * sig * upv);\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "gate",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        },
        {
          "name": "up",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "fast_sigmoid": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "swiglu_flat",
        "vec_width": 2,
        "unroll": 1
      },
      "source_hash": "299fcfaa43eab15032de60da82a02a263704893647cda1d2cc0b2111b920c300",
      "metrics": {
        "latency_us": 195.6455,
        "speedup_vs_ref": 1.2776092473376592,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "swiglu",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "rows": 1024,
          "D": 1536,
          "N": 1572864
        }
      },
      "candidate_id": "swiglu_e79b57774f9d99c2",
      "func_name": "kk_kd_swiglu_n196608_tg128_v2_u1_fast1",
      "metal_source": "constexpr uint N = 196608;\nconstexpr uint VEC = 2;\nconstexpr uint UNROLL = 1;\nconstexpr bool FAST_SIGMOID = true;\n\nuint tid = thread_position_in_grid.x;\nuint base = tid * VEC * UNROLL;\n\n#pragma unroll\nfor (uint u = 0; u < UNROLL; ++u) {\n    uint idx0 = base + u * VEC;\n    if (idx0 >= N) {\n        continue;\n    }\n\n    #pragma unroll\n    for (uint v = 0; v < VEC; ++v) {\n        uint idx = idx0 + v;\n        if (idx >= N) {\n            continue;\n        }\n\n        float g = (float)gate[idx];\n        float upv = (float)up[idx];\n        float sig;\n        if (FAST_SIGMOID) {\n            sig = kk_sigmoid(g);\n        } else {\n            sig = 1.0f / (1.0f + metal::exp(-g));\n        }\n        out[idx] = (T)(g * sig * upv);\n    }\n}\n",
      "inputs_spec": [
        {
          "name": "gate",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        },
        {
          "name": "up",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "out",
          "dtype": "float16",
          "shape": [
            196608
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "vec_width": 2,
        "unroll": 1,
        "fast_sigmoid": true
      },
      "launch_params": {
        "threadgroup_x": 128,
        "launch_kind": "swiglu_flat",
        "vec_width": 2,
        "unroll": 1
      },
      "source_hash": "299fcfaa43eab15032de60da82a02a263704893647cda1d2cc0b2111b920c300",
      "metrics": {
        "latency_us": 195.6455,
        "speedup_vs_ref": 1.2776092473376592,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "rope",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "B": 1,
          "H_Q": 32,
          "D_NOPE": 128,
          "D_ROPE": 64,
          "D_OUT": 192
        }
      },
      "candidate_id": "rope_c73769625b5efd6b",
      "func_name": "kk_kd_rope_dn128_dr64_hq32_tg256_fma1",
      "metal_source": "constexpr uint D_NOPE = 128;\nconstexpr uint D_ROPE = 64;\nconstexpr uint HALF = D_ROPE / 2;\nconstexpr uint D_OUT = D_NOPE + D_ROPE;\nconstexpr uint H_Q = 32;\nconstexpr bool USE_FMA = true;\n\nconstexpr uint Q_ELEMS_PER_BATCH = H_Q * D_OUT;\nconstexpr uint K_ELEMS_PER_BATCH = D_OUT;\nconstexpr uint ELEMS_PER_BATCH = Q_ELEMS_PER_BATCH + K_ELEMS_PER_BATCH;\n\nuint gid = thread_position_in_grid.x;\nuint batch = gid / ELEMS_PER_BATCH;\nuint in_batch = gid - batch * ELEMS_PER_BATCH;\n\nif (in_batch < Q_ELEMS_PER_BATCH) {\n    uint head = in_batch / D_OUT;\n    uint col = in_batch - head * D_OUT;\n\n    uint q_out_base = (batch * H_Q + head) * D_OUT;\n\n    if (col < D_NOPE) {\n        uint q_nope_base = (batch * H_Q + head) * D_NOPE;\n        q_out[q_out_base + col] = q_nope[q_nope_base + col];\n        return;\n    }\n\n    uint r = col - D_NOPE;\n    uint pair = r / 2;\n    float c = (float)cos[pair];\n    float s = (float)sin[pair];\n\n    uint q_rope_base = (batch * H_Q + head) * D_ROPE;\n    if ((r & 1u) == 0u) {\n        float a = (float)q_rope[q_rope_base + r];\n        float b = (float)q_rope[q_rope_base + r + 1];\n        float rotated = USE_FMA ? fma(-b, s, a * c) : (a * c - b * s);\n        q_out[q_out_base + col] = (T)rotated;\n    } else {\n        float a = (float)q_rope[q_rope_base + r - 1];\n        float b = (float)q_rope[q_rope_base + r];\n        float rotated = USE_FMA ? fma(b, c, a * s) : (a * s + b * c);\n        q_out[q_out_base + col] = (T)rotated;\n    }\n    return;\n}\n\nuint k_col = in_batch - Q_ELEMS_PER_BATCH;\nuint k_out_base = batch * D_OUT;\nif (k_col < D_NOPE) {\n    uint kv_nope_base = batch * D_NOPE;\n    k_out[k_out_base + k_col] = kv_nope[kv_nope_base + k_col];\n    return;\n}\n\nuint kr = k_col - D_NOPE;\nuint kpair = kr / 2;\nfloat kc = (float)cos[kpair];\nfloat ks = (float)sin[kpair];\nuint k_rope_base = batch * D_ROPE;\nif ((kr & 1u) == 0u) {\n    float a = (float)k_rope[k_rope_base + kr];\n    float b = (float)k_rope[k_rope_base + kr + 1];\n    float rotated = USE_FMA ? fma(-b, ks, a * kc) : (a * kc - b * ks);\n    k_out[k_out_base + k_col] = (T)rotated;\n} else {\n    float a = (float)k_rope[k_rope_base + kr - 1];\n    float b = (float)k_rope[k_rope_base + kr];\n    float rotated = USE_FMA ? fma(b, kc, a * ks) : (a * ks + b * kc);\n    k_out[k_out_base + k_col] = (T)rotated;\n}\n",
      "inputs_spec": [
        {
          "name": "q_nope",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            128
          ],
          "strides": "contiguous"
        },
        {
          "name": "q_rope",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "kv_nope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            128
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_rope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "cos",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        },
        {
          "name": "sin",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "q_out",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_out",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            192
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "use_fma": true
      },
      "launch_params": {
        "threadgroup_x": 256,
        "launch_kind": "rope_decode_pos"
      },
      "source_hash": "d35718dd7286137f5c16989a8ffdb9dce18e398404be4025aabfc457a3444ffc",
      "metrics": {
        "latency_us": 130.45850000000002,
        "speedup_vs_ref": 1.7480041545778924,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "rope",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "B": 2,
          "H_Q": 32,
          "D_NOPE": 128,
          "D_ROPE": 64,
          "D_OUT": 192
        }
      },
      "candidate_id": "rope_c73769625b5efd6b",
      "func_name": "kk_kd_rope_dn128_dr64_hq32_tg256_fma1",
      "metal_source": "constexpr uint D_NOPE = 128;\nconstexpr uint D_ROPE = 64;\nconstexpr uint HALF = D_ROPE / 2;\nconstexpr uint D_OUT = D_NOPE + D_ROPE;\nconstexpr uint H_Q = 32;\nconstexpr bool USE_FMA = true;\n\nconstexpr uint Q_ELEMS_PER_BATCH = H_Q * D_OUT;\nconstexpr uint K_ELEMS_PER_BATCH = D_OUT;\nconstexpr uint ELEMS_PER_BATCH = Q_ELEMS_PER_BATCH + K_ELEMS_PER_BATCH;\n\nuint gid = thread_position_in_grid.x;\nuint batch = gid / ELEMS_PER_BATCH;\nuint in_batch = gid - batch * ELEMS_PER_BATCH;\n\nif (in_batch < Q_ELEMS_PER_BATCH) {\n    uint head = in_batch / D_OUT;\n    uint col = in_batch - head * D_OUT;\n\n    uint q_out_base = (batch * H_Q + head) * D_OUT;\n\n    if (col < D_NOPE) {\n        uint q_nope_base = (batch * H_Q + head) * D_NOPE;\n        q_out[q_out_base + col] = q_nope[q_nope_base + col];\n        return;\n    }\n\n    uint r = col - D_NOPE;\n    uint pair = r / 2;\n    float c = (float)cos[pair];\n    float s = (float)sin[pair];\n\n    uint q_rope_base = (batch * H_Q + head) * D_ROPE;\n    if ((r & 1u) == 0u) {\n        float a = (float)q_rope[q_rope_base + r];\n        float b = (float)q_rope[q_rope_base + r + 1];\n        float rotated = USE_FMA ? fma(-b, s, a * c) : (a * c - b * s);\n        q_out[q_out_base + col] = (T)rotated;\n    } else {\n        float a = (float)q_rope[q_rope_base + r - 1];\n        float b = (float)q_rope[q_rope_base + r];\n        float rotated = USE_FMA ? fma(b, c, a * s) : (a * s + b * c);\n        q_out[q_out_base + col] = (T)rotated;\n    }\n    return;\n}\n\nuint k_col = in_batch - Q_ELEMS_PER_BATCH;\nuint k_out_base = batch * D_OUT;\nif (k_col < D_NOPE) {\n    uint kv_nope_base = batch * D_NOPE;\n    k_out[k_out_base + k_col] = kv_nope[kv_nope_base + k_col];\n    return;\n}\n\nuint kr = k_col - D_NOPE;\nuint kpair = kr / 2;\nfloat kc = (float)cos[kpair];\nfloat ks = (float)sin[kpair];\nuint k_rope_base = batch * D_ROPE;\nif ((kr & 1u) == 0u) {\n    float a = (float)k_rope[k_rope_base + kr];\n    float b = (float)k_rope[k_rope_base + kr + 1];\n    float rotated = USE_FMA ? fma(-b, ks, a * kc) : (a * kc - b * ks);\n    k_out[k_out_base + k_col] = (T)rotated;\n} else {\n    float a = (float)k_rope[k_rope_base + kr - 1];\n    float b = (float)k_rope[k_rope_base + kr];\n    float rotated = USE_FMA ? fma(b, kc, a * ks) : (a * ks + b * kc);\n    k_out[k_out_base + k_col] = (T)rotated;\n}\n",
      "inputs_spec": [
        {
          "name": "q_nope",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            128
          ],
          "strides": "contiguous"
        },
        {
          "name": "q_rope",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "kv_nope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            128
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_rope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "cos",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        },
        {
          "name": "sin",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "q_out",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_out",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            192
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "use_fma": true
      },
      "launch_params": {
        "threadgroup_x": 256,
        "launch_kind": "rope_decode_pos"
      },
      "source_hash": "d35718dd7286137f5c16989a8ffdb9dce18e398404be4025aabfc457a3444ffc",
      "metrics": {
        "latency_us": 130.45850000000002,
        "speedup_vs_ref": 1.7480041545778924,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    },
    {
      "key": {
        "op_name": "rope",
        "mlx_version": "0.30.6.dev20260208+185b06d9",
        "device_arch": "applegpu_g16s",
        "device_name": "Apple M4 Max",
        "dtype": "float16",
        "shape_signature": {
          "B": 4,
          "H_Q": 32,
          "D_NOPE": 128,
          "D_ROPE": 64,
          "D_OUT": 192
        }
      },
      "candidate_id": "rope_c73769625b5efd6b",
      "func_name": "kk_kd_rope_dn128_dr64_hq32_tg256_fma1",
      "metal_source": "constexpr uint D_NOPE = 128;\nconstexpr uint D_ROPE = 64;\nconstexpr uint HALF = D_ROPE / 2;\nconstexpr uint D_OUT = D_NOPE + D_ROPE;\nconstexpr uint H_Q = 32;\nconstexpr bool USE_FMA = true;\n\nconstexpr uint Q_ELEMS_PER_BATCH = H_Q * D_OUT;\nconstexpr uint K_ELEMS_PER_BATCH = D_OUT;\nconstexpr uint ELEMS_PER_BATCH = Q_ELEMS_PER_BATCH + K_ELEMS_PER_BATCH;\n\nuint gid = thread_position_in_grid.x;\nuint batch = gid / ELEMS_PER_BATCH;\nuint in_batch = gid - batch * ELEMS_PER_BATCH;\n\nif (in_batch < Q_ELEMS_PER_BATCH) {\n    uint head = in_batch / D_OUT;\n    uint col = in_batch - head * D_OUT;\n\n    uint q_out_base = (batch * H_Q + head) * D_OUT;\n\n    if (col < D_NOPE) {\n        uint q_nope_base = (batch * H_Q + head) * D_NOPE;\n        q_out[q_out_base + col] = q_nope[q_nope_base + col];\n        return;\n    }\n\n    uint r = col - D_NOPE;\n    uint pair = r / 2;\n    float c = (float)cos[pair];\n    float s = (float)sin[pair];\n\n    uint q_rope_base = (batch * H_Q + head) * D_ROPE;\n    if ((r & 1u) == 0u) {\n        float a = (float)q_rope[q_rope_base + r];\n        float b = (float)q_rope[q_rope_base + r + 1];\n        float rotated = USE_FMA ? fma(-b, s, a * c) : (a * c - b * s);\n        q_out[q_out_base + col] = (T)rotated;\n    } else {\n        float a = (float)q_rope[q_rope_base + r - 1];\n        float b = (float)q_rope[q_rope_base + r];\n        float rotated = USE_FMA ? fma(b, c, a * s) : (a * s + b * c);\n        q_out[q_out_base + col] = (T)rotated;\n    }\n    return;\n}\n\nuint k_col = in_batch - Q_ELEMS_PER_BATCH;\nuint k_out_base = batch * D_OUT;\nif (k_col < D_NOPE) {\n    uint kv_nope_base = batch * D_NOPE;\n    k_out[k_out_base + k_col] = kv_nope[kv_nope_base + k_col];\n    return;\n}\n\nuint kr = k_col - D_NOPE;\nuint kpair = kr / 2;\nfloat kc = (float)cos[kpair];\nfloat ks = (float)sin[kpair];\nuint k_rope_base = batch * D_ROPE;\nif ((kr & 1u) == 0u) {\n    float a = (float)k_rope[k_rope_base + kr];\n    float b = (float)k_rope[k_rope_base + kr + 1];\n    float rotated = USE_FMA ? fma(-b, ks, a * kc) : (a * kc - b * ks);\n    k_out[k_out_base + k_col] = (T)rotated;\n} else {\n    float a = (float)k_rope[k_rope_base + kr - 1];\n    float b = (float)k_rope[k_rope_base + kr];\n    float rotated = USE_FMA ? fma(b, kc, a * ks) : (a * ks + b * kc);\n    k_out[k_out_base + k_col] = (T)rotated;\n}\n",
      "inputs_spec": [
        {
          "name": "q_nope",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            128
          ],
          "strides": "contiguous"
        },
        {
          "name": "q_rope",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "kv_nope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            128
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_rope",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            64
          ],
          "strides": "contiguous"
        },
        {
          "name": "cos",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        },
        {
          "name": "sin",
          "dtype": "float16",
          "shape": [
            32
          ],
          "strides": "contiguous"
        }
      ],
      "outputs_spec": [
        {
          "name": "q_out",
          "dtype": "float16",
          "shape": [
            1,
            32,
            1,
            192
          ],
          "strides": "contiguous"
        },
        {
          "name": "k_out",
          "dtype": "float16",
          "shape": [
            1,
            1,
            1,
            192
          ],
          "strides": "contiguous"
        }
      ],
      "template_params": {
        "use_fma": true
      },
      "launch_params": {
        "threadgroup_x": 256,
        "launch_kind": "rope_decode_pos"
      },
      "source_hash": "d35718dd7286137f5c16989a8ffdb9dce18e398404be4025aabfc457a3444ffc",
      "metrics": {
        "latency_us": 130.45850000000002,
        "speedup_vs_ref": 1.7480041545778924,
        "correctness_max_abs_err": 0.0,
        "correctness_max_rel_err": 0.0
      }
    }
  ]
}